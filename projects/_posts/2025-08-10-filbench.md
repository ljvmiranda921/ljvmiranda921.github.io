---
layout: post
type: post
title: "Introducing FilBench: An LLM Evaluation Suite for Filipino"
date: 2025-08-10
category: projects
comments: true
author: "LJ MIRANDA"
published: true
tags:
  [
    nlp,
    language technology,
    natural language processing,
    tagalog,
    low resource,
    llm,
    machine learning,
  ]
description: |
  This Buwan ng Wika (National Language Month), I'm proud to introduce FilBench, a big step forward in Filipino NLP evaluation.
  Read to learn more!
excerpt: |
  This Buwan ng Wika (National Language Month), I'm proud to introduce FilBench, a big step forward in Filipino NLP evaluation.
  Read to learn more!
---

<span class="firstcharacter">A</span>t the end of 2024, I wrote about my [desiderata for Filipino NLP](/notebook/2024/12/17/filipino-llm/).
One of which was **evaluation**.
I said that most of "how we measure LLM capabilities in Filipino are anecdotal: we post a screenshot of ChatGPT writing in Filipino and claim that it already has that capability&mdash;we need a systematic approach to evaluating these models."
Fast forward to today, I'm happy that we are now inching towards systematic evaluations for Filipino.

Without further ado, I introduce **FilBench**, an LLM Evaluation Suite for Filipino!

<iframe
	src="https://mteb-leaderboard.hf.space"
	frameborder="0"
	width="700"
	height="450"
></iframe>

&nbsp;

## What is FilBench?

FilBench is a (1) **benchmark** to test LLM capabilities on Filipino, and a (2) **leaderboard** to track the progress of LLM development for Philippine languages.
When building FilBench, I imagine two types of audiences:

- The **multilingual NLP research scientist** who wants to test whether their new language model generalizes to other languages.
  FilBench aims to provide a robust and comprehensive evaluation suite for Filipino tasks and use-cases.

- The **language model developer** who wants to know which language model fits best for the application their building.
  The FilBench leaderboard provides a detailed breakdown of a model's capabilities, and analyses of the parameter- and cost-efficiency of such models.

<!-- ## Why do we need FilBench?

#### FilBench fills the need for systematic evaluations for Filipino




#### We need more language-specific benchmarks




## What's unique about FilBench?

<!-- I am excited to see that the timing of my [*Desiderata* blog post](/notebook/2024/12/17/filipino-llm/) was on point: after posting it, I saw the release of several benchmarks that target LLM capabilities on Filipino (or Southeast Asian language) capabilities.
Notable examples include [SEA-HELM](), and its Tagalog subset, [Batayan]().
This shows that the research pulse right now is on evaluation.

Compared to these benchmarks, FilBench aims to be more opinionated on what it's testing.
We start by taking stock of what many Filipino NLP researchers are evaluating pre-GPT language models upon, and creating a taxonomy of capabilities consisting of Classical NLP, Cultural Knowledge, Reading Comprehension, and Generation. -->

<!--
#### FilBench is informed by the priorities of the PH NLP research community

#### FilBench is a uniquely grassroots Effort
-->

<!-- One thing that's really exciting about building FilBench is that I'm like assembling the Avengers of Filipino NLP.
It actually started with just the three of us&mdash; Ely, Conner, and I&mdash; working on the [Data Is Better Together annotation project](https://github.com/huggingface/data-is-better-together) from HuggingFace.
I've met them separately through other projects like [SEACrowd](https://seacrowd.github.io/) and earlier correspondences. -->

<!--

## What did we learn from building FilBench?


## What's next? -->


