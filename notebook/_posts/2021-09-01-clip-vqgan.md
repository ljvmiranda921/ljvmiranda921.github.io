---
layout: post
type: post
title: "Taming the 'Taming Transformers' (VQGAN) paper"
date: 2021-09-01
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [vqgan, machine learning, research, deep learning, neural network, clip vqgan]
description: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
excerpt: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
---

<span class="firstcharacter">T</span>**ext-to-image synthesis has taken ML
Twitter by storm**. Everyday, we see new AI-generated artworks being shared
across our feeds. All of these were made possible thanks to the [VQGAN-CLIP
Colab
Notebook](https://colab.research.google.com/drive/1_4Jl0a7WIJeqy5LTjPJfZOwMZopG5C-W?usp=sharing#scrollTo=g7EDme5RYCrt)
of [@advadnoun](https://twitter.com/advadnoun) and
[@RiversHaveWings](https://twitter.com/RiversHaveWings). They were able to
combine the generative capabilities of VQGAN ([Esser et al,
2021](#esser2021vqgan)) and discriminative ability of CLIP ([Radford et al,
2021](#radford2021clip)) to produce the wonderful images we see today:

![](/assets/png/vqgan/ai_art_collage.png){:width="540px"}  
<br>
__Figure:__ *A few images generated by VQGAN-CLIP. Featuring works by
[@advadnoun](https://twitter.com/advadnoun),
[@RiversHaveWings](https://twitter.com/RiversHaveWings), and [Ryan
Moulton](https://moultano.wordpress.com/2021/07/20/tour-of-the-sacred-library/).
Also includes some works from [@images_ai](https://twitter.com/images_ai).*
{: style="text-align: center; margin: 1.5em"}

First things first: VQGAN stands for Vector Quantized Generative Adversarial
Network, while CLIP stands for Contrastive Image-Language Pretraining. Whenever
we say VQGAN-CLIP[^1], we refer to the interaction between these two networks.
They're separate models that work in tandem.

In essence, the way they work is that VQGAN generates the images, while CLIP
judges how well an image matches our text prompt. This interaction guides our
generator to produce more *accurate* images:

![](/assets/png/vqgan/clip_vqgan_with_image.png){:width="540px"}  
<br>
__Figure:__ *The VQGAN model generates images while CLIP guides the process. This is
done throughout many iterations until the generator learns to produce more
"accurate" images.*
{: style="text-align: center; margin: 1.5em"}

**However, I'm more interested in how VQGAN works.** It seems to prescribe a
theory of perception that I find interesting. That's why I'll be focusing on
the paper, ["Taming Transformers for high-resolution images
synthesis."](https://arxiv.org/abs/2012.09841) On the other hand, if you wish
to learn more about CLIP, I suggest reading [OpenAI's
explainer](https://openai.com/blog/clip/)&mdash;it's comprehensive and accessible. 


<div style="border:3px; border-style:solid; border-color:#a00000; padding: 1em;">
<b>Contents</b><br>
<ul>
    <li><a href="#perception">How we see images: a theory of perception</a></li>
    <li>Using Transformers to model interactions</li>
    <li>Expressing modalities through a codebook</li>
    <li>Training the codebook using a GAN</li>
    <li>Cost-function tricks</li>
</ul>
<b>Appendix</b>
<ol>
    <li>Literature leading to VQGAN</li>
    <li>Revisiting CLIP</li>
</ol>
</div>

<br>


## <a id="perception"></a> How we see images: a theory of perception

One thing that I like about VQGAN is that it prescribes an explanation of how
we see things&mdash; a *theory of perception*, if you may. As a motivating
example, if I ask you to describe this picture below, what would you say?

![](/assets/png/vqgan/lenna_test_image.png){:width="360px"}  
<br>
__Figure:__ *If I ask you to describe this picture, what would you say?*
{: style="text-align: center; margin: 1.5em"}

Some of you may describe this as *"a lady with a feathered hat looking back,"*
or *"a woman with a hat."* More discerning folks in the field might describe
this as *"the Lenna standard test image in computer vision."*  Nevertheless, we
seem to encounter images through discrete representations: `woman`, `feathered
hat`, or `Lenna`. This theory of perception suggests that **our visual
reasoning is symbolic, we ascribe meaning through discrete representations and
modalities.**

> This theory of perception suggests that our visual reasoning is symbolic, we
> ascribe meaning through discrete representations and modalities.

This symbolic approach allows us to understand relationships between different
words or symbols. In literature, this is commonly known as being able to **model
[long-range
dependencies](https://en.wikipedia.org/wiki/Long-range_dependence)**.
For example, in the sentence "a lady with a feathered hat looking back," we
knew that `looking back` refers to the `lady`'s action while `feathered`
refers to the `hat`'s description.

![](/assets/png/vqgan/lrd.png){:width="480px"}  
<br>
__Figure:__ *A discrete representation allows us to understand
relationships across symbols   
or model long-range dependencies*
{: style="text-align: center; margin: 1.5em"}

Even though a lot of work has been done to explore complex reasoning through discrete
representations ([Salakhutdinov and Hinton, 2009](#salakhutdinov2009boltzmann),
[Mnih and Gregor, 2014](#mnih2014neural), and [Oord et al,
2017](#oord2017discrete)), **most computer vision (CV)
techniques don't think in terms of modalities.** Instead, they think in terms of pixels:

![](/assets/png/vqgan/lenna_pixels.png){:width="320px"}  
<br>
__Figure:__ *A close-up of the Lenna image. Each square represents a pixel with
three channels: red, green, and blue. Each channel has a continuous value and
affects the resulting color of the pixel.*
{: style="text-align: center; margin: 1.5em"}

Instead of thinking in terms of large chunks of information, common CV
techniques focus on the smallest unit of an image. Each pixel has three
channels&mdash;red, green, blue&mdash; that represent its color value in a
continuous scale. Needless to say, this *may* not be how our perception
works.[^2]

Despite all of these, we still shouldn't ignore pixel-based approaches.
Convolutional neural networks (CNN) proved that we can model local interactions
between pixels, simply by restricting interactions within their local
neighborhood (also known as the [*kernel*](https://en.wikipedia.org/wiki/Kernel_(image_processing))). This allows us to "compose" pixels together and learn visual
parts ([Gu et al, 2018](#gu2018cnn)).  The premier illustration for this is the
feature map below, where a CNN learned how to compose pixels at varying layers
of abstraction: pixels become edges, edges become shapes, and shapes become
parts.

![](/assets/png/vqgan/cnn_features.png){:width="520px"}  
<br>
__Figure:__ *A convolutional neural network feature map showing features at
different levels   
(photo from [Tejpal Virdi, 2017](https://tvirdi.github.io/2017-10-29/cnn/))*
{: style="text-align: center; margin: 1.5em"}

Putting it all together, we now have: 
1. an interesting view of perception that allows us to **model
    long-range dependencies** by representing images discretely; and&mdash;
2. a pixel-based approach that we shouldn't ignore in order to take advantage of
    learned **local interactions** and **visual parts**.

**VQGAN was able to combine these two approaches.** It can learn not only the
(1) visual parts of an image, but also the (2) relationship (read: long-range
dependencies) between these parts. We knew that the former can be done by a
convolutional neural network, but we still have to discuss the latter.
The table below summarizes the two: 

| Approach   | Examples                                             | Can model                           | Analogy[^4]    |
|------------|------------------------------------------------------|-------------------------------------|------------|
| Discrete   | Sequence of symbols, words, phrases, and sentences   | Long-range dependencies             | Perceiving |
| Continuous[^3] | RGB channels in a pixel, convolutional filters, etc. | Local interactions and visual parts | Sensing    |


> VQGAN can learn not only the visual parts of an image, but also their relationships

In the next section, we'll talk about how a Transformer ([Vaswani et al,
2017](#vaswani2017attention)) can model long-range dependencies between
symbols. Transformers have been ubiquitous in natural-language processing, and
seems to be a nice fit for modelling our theory of perception. **However, it has
one weakness: it doesn't scale well to images.**

## Using Transformers to model interactions

In the previous section, we introduced two approaches for handling images: (1)
a continuous approach that learns visual parts using a convolutional neural
network&mdash; 

![](/assets/png/vqgan/cnn_diagram.png){:width="520px"}  
<br>
__Figure:__ *Given an image dataset, a convolutional neural network can learn
high-level features that we refer to as visual parts.*
{: style="text-align: center; margin: 1.5em"}


and (2) a discrete approach that learns long-range dependencies across visual parts. We've
also alluded that the latter is done by a Transformer network. Finally, we
mentioned that VQGAN was able to take advantage of the two approaches. 

The logical next step is to combine them together. One way is to directly feed
the feature map into a Transformer. We can flatten the pixels of each visual
part into a sequence and use that as input: 


![](/assets/png/vqgan/flatten_pixels.png){:width="720px"}  
<br>
__Figure:__ *We can flatten the learned visual parts into a sequence and feed
it into a Transformer network (Note that the 4x4 size in the figure is illustrative).*
{: style="text-align: center; margin: 1.5em"}


[Chen et al (2020)](#chen2020pixels) has explored this approach. However, they
encountered a limitation in the transformer network: it scales quadratically
with the length of the input sequence. A 224 x 224 px image will have a length
of $$224^2 \times 3$$, way above the capacity of a GPU. As a result, they
reduced the context by downsampling the 224-px image to 32-, 48-, and 64 pixel
dimensions.

The reason Transformers scale quadratically is because they have to compute the
pairwise interaction between all elements...

<!-- maybe image of a transformer doing a compute -->


There have been many attempts to circumvent the scaling issue, but it paid the
cost of not being able to synthesize high-resolution imagery.

<!-- image of a broken chain in your diagram -->

<!-- so what did VQGAN do? -->


<!-- transformers have been proven to be good at long sequences and capture
long-range dependencies, we should take advantage of that -->

<!-- However, the problem is that using transformers is not scaleable.
Everything is still in pixels -->


## Expressing modalities through a codebook 


## Training the codebook using a GAN (w/ VQ)

## Cost-function tricks


## References

1. <a id="chen2020pixels">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D. and Sutskever, I.</a>, 2020, November. Generative pretraining from pixels. In International Conference on Machine Learning (pp. 1691-1703). PMLR.
1. <a id="esser2021vqgan">Esser, P., Rombach, R. and Ommer, B.</a>, 2021. Taming transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 12873-12883).
1. <a id="gu2018cnn">Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J. and Chen, T.</a>, 2018. Recent advances in convolutional neural networks. Pattern Recognition, 77, pp.354-377.
1. <a id="mnih2014neural">Mnih, A. and Gregor, K.</a>, 2014, June. Neural variational inference and learning in belief networks. In International Conference on Machine Learning (pp. 1791-1799). PMLR.
1. <a id="oord2017discrete">Oord, A.V.D., Vinyals, O. and Kavukcuoglu, K.</a>, 2017. Neural discrete representation learning. arXiv preprint arXiv:1711.00937.
1. <a id="radford2021clip">Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G.</a>, 2021. Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*.
1. <a id="salakhutdinov2009boltzmann">Salakhutdinov, R. and Hinton, G., 2009</a>, April. Deep boltzmann machines. In Artificial intelligence and statistics (pp. 448-455). PMLR.
1. <a id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.</a>, 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).

## Footnotes

[^1]: You might see VQGAN-CLIP being written as CLIP-VQGAN, CLIP+VQGAN, or VQGAN+CLIP. The order doesn't matter and the dash symbol isn't an operation. They're all the same thing.
[^2]: It may not be how we perceive the world around us, but it may be how we *sense* it. Note that our eyes are composed of [*cone cells*](https://en.wikipedia.org/wiki/Cone_cell) that respond differently to different wavelengths. We can treat these cones analogous to a pixel's RGB channels.
[^3]: I labeled the pixel-based approach as continuous to complete the story. When normalized, you can think of pixel values as a number between 0 to 1, each representing the intensity or presence of that color.
[^4]: I will admit that this analogy may be a stretch. However, I'd like to think that even if we reason in a symbolic manner, the way information travels to us is through a continuous variation of light wavelengths. As they say, analogies work until they don't. I may be stretching it a bit far in this column.
