---
layout: post
type: post
title: "Taming the 'Taming Transformers' (VQGAN) paper"
date: 2021-09-01
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [vqgan, machine learning, research, deep learning, neural network, clip vqgan]
description: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
excerpt: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
---

<span class="firstcharacter">T</span>**ext-to-image synthesis has taken ML
Twitter by storm**. Everyday, we see new AI-generated artworks being shared
across our feeds. All of these were made possible thanks to the [VQGAN-CLIP
Colab
Notebook](https://colab.research.google.com/drive/1_4Jl0a7WIJeqy5LTjPJfZOwMZopG5C-W?usp=sharing#scrollTo=g7EDme5RYCrt)
of [@advadnoun](https://twitter.com/advadnoun) and
[@RiversHaveWings](https://twitter.com/RiversHaveWings). They were able to
combine the generative capabilities of VQGAN ([Esser et al,
2021](#esser2021vqgan)) and discriminative ability of CLIP ([Radford et al,
2021](#radford2021clip)) to produce the wonderful images we see today:

![](/assets/png/vqgan/ai_art_collage.png){:width="540px"}  
<br>
__Figure:__ *A few images generated by VQGAN-CLIP. Featuring works by
[@advadnoun](https://twitter.com/advadnoun),
[@RiversHaveWings](https://twitter.com/RiversHaveWings), and [Ryan
Moulton](https://moultano.wordpress.com/2021/07/20/tour-of-the-sacred-library/).
Also includes some works from [@images_ai](https://twitter.com/images_ai).*
{: style="text-align: center; margin: 1.5em"}

First things first: VQGAN stands for Vector Quantized Generative Adversarial
Network, while CLIP stands for Contrastive Image-Language Pretraining. Whenever
we say VQGAN-CLIP[^1], we refer to the interaction between these two networks.
They're separate models that work in tandem.

In essence, the way they work is that VQGAN generates the images, while CLIP
judges how well an image matches our text prompt. This interaction guides our
generator to produce more *accurate* images:

![](/assets/png/vqgan/clip_vqgan_with_image.png){:width="540px"}  
<br>
__Figure:__ *The VQGAN model generates images while CLIP guides the process. This is
done throughout many iterations until the generator learns to produce more
"accurate" images.*
{: style="text-align: center; margin: 1.5em"}

**However, I'm more interested in how VQGAN works.** It seems to prescribe a
theory of perception that I find interesting. That's why I'll be focusing on
the paper, ["Taming Transformers for high-resolution images
synthesis."](https://arxiv.org/abs/2012.09841) On the other hand, if you wish
to learn more about CLIP, I suggest reading [OpenAI's
explainer](https://openai.com/blog/clip/)&mdash;it's comprehensive and accessible. 


<div style="border:3px; border-style:solid; border-color:#a00000; padding: 1em;">
<b>Contents</b><br>
<ul>
    <li><a href="#perception">How we see images: a theory of perception</a></li>
    <li><a href="#transformers">Using Transformers to model interactions</a></li>
    <li><a href="#codebook">Expressing images through a codebook</a></li>
    <li><a href="#training">Training VQGAN</a></li>
    <ul>
        <li><a href="#training-gan">Training the GAN</a></li>
        <li><a href="#training-transformer">Training the Transformer</a></li>
    </ul>
    <li><a href="#conclusion">Conclusion</a></li>
</ul>
<b>Appendix</b>
<ol>
    <li>Literature leading to VQGAN</li>
</ol>
</div>

<br>


## <a id="perception"></a> How we see images: a theory of perception

One thing that I like about VQGAN is that it prescribes an explanation of how
we see things&mdash; a *theory of perception*, if you may. As a motivating
example, if I ask you to describe this picture below, what would you say?

![](/assets/png/vqgan/lenna_test_image.png){:width="360px"}  
<br>
__Figure:__ *If I ask you to describe this picture, what would you say?*
{: style="text-align: center; margin: 1.5em"}

Some of you may describe this as *"a lady with a feathered hat looking back,"*
or *"a woman with a hat."* More discerning folks in the field might describe
this as *"the Lenna standard test image in computer vision."*  Nevertheless, we
seem to encounter images through discrete representations: `woman`, `feathered
hat`, or `Lenna`. This theory of perception suggests that **our visual
reasoning is symbolic, we ascribe meaning through discrete representations and
modalities.**

> This theory of perception suggests that our visual reasoning is symbolic, we
> ascribe meaning through discrete representations and modalities.

This symbolic approach allows us to understand relationships between different
words or symbols. In machine learning, this is commonly known as being able to
**model [long-range
dependencies](https://en.wikipedia.org/wiki/Long-range_dependence)**.  For
example, in the sentence "a lady with a feathered hat looking back," we knew
that `looking back` refers to the `lady`'s action while `feathered` refers to
the `hat`'s description.

![](/assets/png/vqgan/lrd.png){:width="480px"}  
<br>
__Figure:__ *A discrete representation allows us to understand
relationships across symbols   
or model long-range dependencies*
{: style="text-align: center; margin: 1.5em"}

Even though a lot of work has been done to explore complex reasoning through discrete
representations ([Salakhutdinov and Hinton, 2009](#salakhutdinov2009boltzmann),
[Mnih and Gregor, 2014](#mnih2014neural), and [Oord et al,
2017](#oord2017discrete)), **most computer vision (CV)
techniques don't think in terms of modalities.** Instead, they think in terms of pixels:

![](/assets/png/vqgan/lenna_pixels.png){:width="320px"}  
<br>
__Figure:__ *A close-up of the Lenna image. Each square represents a pixel with
three channels: red, green, and blue. Each channel has a continuous value and
affects the resulting color of the pixel.*
{: style="text-align: center; margin: 1.5em"}

Instead of thinking in terms of large chunks of information, common CV
techniques focus on the smallest unit of an image. Each pixel has three
channels&mdash;red, green, blue&mdash; that represent its color value in a
continuous scale. Needless to say, this *may* not be how our perception
works.[^2]

Despite all of these, we still shouldn't ignore pixel-based approaches.
Convolutional neural networks (CNN) proved that we can model local interactions
between pixels, simply by restricting interactions within their local
neighborhood (also known as the [*kernel*](https://en.wikipedia.org/wiki/Kernel_(image_processing))). This allows us to "compose" pixels together and learn visual
parts ([Gu et al, 2018](#gu2018cnn)).  The premier illustration for this is the
feature map below, where a CNN learned how to compose pixels at varying layers
of abstraction: pixels become edges, edges become shapes, and shapes become
parts.

![](/assets/png/vqgan/cnn_features.png){:width="520px"}  
<br>
__Figure:__ *A convolutional neural network feature map showing features at
different levels   
(photo from [Tejpal Virdi, 2017](https://tvirdi.github.io/2017-10-29/cnn/))*
{: style="text-align: center; margin: 1.5em"}

Putting it all together, we now have two complementary techniques: 
1. an interesting view of perception that allows us to **model
    long-range dependencies** by representing images discretely; and&mdash;
2. a pixel-based approach that we shouldn't ignore in order to take advantage of
    learned **local interactions** and **visual parts**.

**VQGAN was able to combine both of them.** It can learn not only the
(1) visual parts of an image, but also the (2) relationship (read: long-range
dependencies) between these parts. We knew that the former can be done by a
convolutional neural network, but we still have to discuss the latter.
The table below summarizes the two: 

| Approach   | Examples                                             | Can model                           | Analogy[^4]    |
|------------|------------------------------------------------------|-------------------------------------|------------|
| Discrete   | Sequence of symbols, words, phrases, and sentences   | Long-range dependencies             | Perceiving |
| Continuous[^3] | RGB channels in a pixel, convolutional filters, etc. | Local interactions and visual parts | Sensing    |


> VQGAN can learn not only the visual parts of an image, but also their relationships

In the next section, we'll talk about how a Transformer ([Vaswani et al,
2017](#vaswani2017attention)) can model long-range dependencies between
symbols. Transformers have been ubiquitous in natural-language processing, and
seems to be a nice fit for modelling our theory of perception. **However, it has
one weakness: it doesn't scale well to images.**

## <a id="transformers"></a> Using Transformers to model interactions

In the previous section, we introduced two approaches for handling images: (1)
a continuous approach that learns visual parts using a convolutional neural
network&mdash; 

![](/assets/png/vqgan/cnn_diagram.png){:width="520px"}  
<br>
__Figure:__ *Given an image dataset, a convolutional neural network can learn
high-level features that we refer to as visual parts.*
{: style="text-align: center; margin: 1.5em"}


and (2) a discrete approach that learns long-range dependencies across visual parts. We've
also alluded that the latter is done by a Transformer network. Finally, we
mentioned that VQGAN was able to take advantage of the two approaches. 

The logical next step is to combine them together. One way is to directly feed
the feature map into a Transformer. We can flatten the pixels of each visual
part into a sequence and use that as input: 


![](/assets/png/vqgan/flatten_pixels.png){:width="720px"}  
<br>
__Figure:__ *We can flatten the learned visual parts into a sequence and feed
it into a Transformer network (Note that the 4x4 size in the figure is illustrative).*
{: style="text-align: center; margin: 1.5em"}

[Chen et al (2020)](#chen2020pixels) explored this approach. However, they
encountered a limitation in the transformer network: its computation scales
quadratically with the length of the input sequence. A 224 x 224 px image will
have a length of $$224^2 \times 3$$, way above the capacity of a GPU. As a
result, they reduced the context by downsampling the 224-px image to 32-, 48-,
and 64 pixel dimensions.

The reason Transformers scale quadratically is because of its attention
mechanism ([Vaswani et al, 2017](#vaswani2017attention)), where it computes for
the pairwise inner product between each pair of the tokenized words. Through
this method, it can learn about the long-range dependencies between tokens.

![](/assets/png/vqgan/transformer_diagram.png){:width="580px"}  <br>
__Figure:__ *Transfromer works through its attention mechanism. It's a
quadratic operation that scales with the length of the input sequence*
{: style="text-align: center; margin: 1.5em"}

There have been many attempts to circumvent the scaling issue, but at the cost
of not being able to synthesize high-resolution imagery or making assumptions
on pixel information. These were done by restricting the receptive fields of
the attention modules ([Parmar et al, 2018](#parmar2018transformer) and
[Weiseenborn et al, 2019](weissenborn2019video)), using sparse networks ([Child
et al, 2019](#child2019sparse)), or training from image patches ([Dosovitskiy
et al, 2020](#dosovitskiy2020vit)). Nevertheless, we see a two-stage approach
common across all works. 

![](/assets/png/vqgan/two_stage_v0.png){:width="720px"}  
<br>
__Figure:__ *Common to most techniques is a two-stage approach that first learns
a representation from the image and encodes it to an intermediary form before
feeding into a transformer (or any autoregressive network).*
{: style="text-align: center; margin: 1.5em"}


**VQGAN employs the same two-stage structure, where it learns an intermediary
representation before feeding it to a transformer.** However, instead of
downsampling the image, VQGAN uses a **codebook** to represent visual parts.
The authors did not model the image from a pixel-level directly, but instead
from the *codewords* of the learned codebook.

> VQGAN did not model the image from a pixel-level directly, but instead from
> the codewords of the learned codebook.

This codebook is created by performing vector quantization (VQ), and we'll
discuss it more in the next section.

## <a id="codebook"></a> Expressing images through a codebook 

In the previous section, we mentioned that VQGAN was able to solve
Transformer's scaling problem by using an intermediate representation known as
a *codebook*. This codebook serves as the bridge for the two-stage approach
found in most image transformer techniques:


![](/assets/png/vqgan/two_stage_v1.png){:width="720px"}  
<br>
__Figure:__ *VQGAN employs a codebook as an intermediary representation before
feeding it to a transformer network. The codebook is then learned using vector
quantization (VQ).*
{: style="text-align: center; margin: 1.5em"}

**The codebook is generated through a process called vector quantization
(VQ)**, i.e., the "VQ" part of "VQGAN." Vector quantization is a signal
processing technique for encoding vectors. In VQGAN, this process was used to
create a quantized representation of various visual parts. 
Once in discrete form, it is now straightforward and less computationally
expensive to pass it to a transformer network.

One can think of vector quantization as a **process of dividing vectors into
groups that have approximately the same number of points closest to them**
([Ballard, 1999](#ballard1999vq)). Each group is then represented by a centroid
(*codeword*), usually obtained via
[k-means](https://en.wikipedia.org/wiki/K-means_clustering) or any other
[clustering algorithm](https://en.wikipedia.org/wiki/Cluster_analysis). In the
end, one learns a dictionary of centroids (*codebook*)  and their corresponding
members.

![](/assets/png/vqgan/codebook.png){:width="720px"}  
<br>
__Figure:__ *Vector quantization is a classic signal processing technique that
finds   
the representative centroids for each cluster.*
{: style="text-align: center; margin: 1.5em"}

On a conceptual (and admittedly, handwavy) level, we can think of these
codewords as the discrete symbols that we used in our earlier examples: `lady`,
`feathered hat`, `night`, `moon`, `city`, or `rain`. By training them with a
transformer is when we start to uncover their relationships: "there's moon at
night," "lady wears a hat on her head," or "it's cloudy when it rains."[^5]


## <a href="training"></a> Training VQGAN

At this point, we now have all the ingredients needed to discuss how VQGAN is
trained:

*  A **convolutional neural network to learn an image's visual parts.** Later,
      we'll talk about generative adversarial networks (GAN), a CNN architecture that
      allows learning of higher-quality visual features.
* A **transformer network to learn long-range dependencies.** Given a discrete
       representation, a transformer allows us to understand relationships across
       visual parts. 
* A **codebook obtained via vector quantization.** It consists of discrete
       codewords that allows us to easily train a transformer on top of it.

Again, these three components make up a **two-stage approach** as seen in the
figure below:


Training also happens in two stages:

1. Training the GAN from a dataset of images to learn not only its visual
   parts, but also their codeword representation, i.e., the codebook.
2. Training the Transformer on top of the codebook with sliding attention to
   learn long-range interactions across visual parts. 


### <a href="training-gan"></a> Training the GAN

### <a href="training-transformer"></a> Training the transformer


## <a href="conclusion"></a> Conclusion



## References

1. <a id="ballard1999vq">Ballard, D.H.</a>, 1999. An introduction to natural computation. MIT press.
1. <a id="chen2020pixels">Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D. and Sutskever, I.</a>, 2020, November. Generative pretraining from pixels. In International Conference on Machine Learning (pp. 1691-1703). PMLR.
1. <a id="child2019sparse">Child, R., Gray, S., Radford, A. and Sutskever, I.</a>, 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509.
1. <a id="dosovitskiy2020vit">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. and Uszkoreit, J.</a>, 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.
1. <a id="esser2021vqgan">Esser, P., Rombach, R. and Ommer, B.</a>, 2021. Taming transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 12873-12883).
1. <a id="gu2018cnn">Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J. and Chen, T.</a>, 2018. Recent advances in convolutional neural networks. Pattern Recognition, 77, pp.354-377.
1. <a id="mnih2014neural">Mnih, A. and Gregor, K.</a>, 2014, June. Neural variational inference and learning in belief networks. In International Conference on Machine Learning (pp. 1791-1799). PMLR.
1. <a id="oord2017discrete">Oord, A.V.D., Vinyals, O. and Kavukcuoglu, K.</a>, 2017. Neural discrete representation learning. arXiv preprint arXiv:1711.00937.
1. <a id="parmar2018transformer">Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A. and Tran</a>, D., 2018, July. Image transformer. In International Conference on Machine Learning (pp. 4055-4064). PMLR.
1. <a id="radford2021clip">Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G.</a>, 2021. Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*.
1. <a id="salakhutdinov2009boltzmann">Salakhutdinov, R. and Hinton, G., 2009</a>, April. Deep boltzmann machines. In Artificial intelligence and statistics (pp. 448-455). PMLR.
1. <a id="weissenborn2019video">Weissenborn, D., Täckström, O. and Uszkoreit, J.</a>, 2019. Scaling autoregressive video models. arXiv preprint arXiv:1906.02634.
1. <a id="vaswani2017attention">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.</a>, 2017. Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).

## Footnotes

[^1]: You might see VQGAN-CLIP being written as CLIP-VQGAN, CLIP+VQGAN, or VQGAN+CLIP. The order doesn't matter and the dash symbol isn't an operation. They're all the same thing.
[^2]: It may not be how we perceive the world around us, but it may be how we *sense* it. Note that our eyes are composed of [*cone cells*](https://en.wikipedia.org/wiki/Cone_cell) that respond differently to different wavelengths. We can treat these cones analogous to a pixel's RGB channels.
[^3]: I labeled the pixel-based approach as continuous to complete the story. When normalized, you can think of pixel values as a number between 0 to 1, each representing the intensity or presence of that color.
[^4]: I will admit that this analogy may be a stretch. However, I'd like to think that even if we reason in a symbolic manner, the way information travels to us is through a continuous variation of light wavelengths. As they say, analogies work until they don't. I may be stretching it a bit far in this column.
[^5]: Again, treat this paragraph with a grain of salt. It's difficult to truly interpret (i.e., rigorously and empirically) what each codeword represents, moreso their interactions once fed to a transformer. For explanation's sake, just know that at the end of the vector quantization process, we now have discrete inputs that can easily scale with our transformer network.
