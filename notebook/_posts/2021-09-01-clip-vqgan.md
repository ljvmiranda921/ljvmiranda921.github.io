---
layout: post
type: post
title: "Taming the 'Taming Transformers' (VQGAN) paper"
date: 2021-09-01
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [vqgan, machine learning, research, deep learning, neural network, clip vqgan]
description: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
excerpt: |
    The Vector Quantized Generative Adversarial Network (VQGAN) is a network
    that allows us to create high-resolution images from text. Let me talk
    about how it works in this post.
---

<span class="firstcharacter">T</span>**ext-to-image synthesis has taken ML
Twitter by storm**. Everyday, we see new AI-generated artworks being shared
across the interwebs. All of these were made possible thanks to the [VQGAN-CLIP
Colab
Notebook](https://colab.research.google.com/drive/1_4Jl0a7WIJeqy5LTjPJfZOwMZopG5C-W?usp=sharing#scrollTo=g7EDme5RYCrt)
of [@advadnoun](https://twitter.com/advadnoun) and
[@RiversHaveWings](https://twitter.com/RiversHaveWings). They were able to
combine the generative capabilities of VQGAN ([Esser et al, 
2021](#esser2021vqgan)) and discriminative ability of CLIP ([Radford et al,
2021](#radford2021clip)) to produce the wonderful images we see today:

![](/assets/png/vqgan/ai_art_collage.png){:width="540px"}  
<br>
__Figure:__ *A few images generated by VQGAN-CLIP. Featuring works by
[@advadnoun](https://twitter.com/advadnoun),
[@RiversHaveWings](https://twitter.com/RiversHaveWings), and [Ryan
Moulton](https://moultano.wordpress.com/2021/07/20/tour-of-the-sacred-library/).
Also includes some works from [@images_ai](https://twitter.com/images_ai).*
{: style="text-align: center; margin: 1.5em"}

First things first: VQGAN stands for Vector Quantized Generative Adversarial
Network, while CLIP stands for Contrastive Image-Language Pretraining. Whenever
we say VQGAN-CLIP, we refer to the interaction between the two networks.

In essence, the way they work is that VQGAN generates the images, while CLIP
judges how well the image matches our text prompt. This interaction guide our
generator to produce more *accurate* images:

![](/assets/png/vqgan/clip_vqgan_with_image.png){:width="540px"}  
<br>
__Figure:__ *The VQGAN model generates images while CLIP guides the process. This is
done throughout many iterations until the generator learns to produce more
"accurate" images.*
{: style="text-align: center; margin: 1.5em"}

**However, I'm more interested in how VQGAN works.** It seems to prescribe a
theory of perception that I find interesting. That's why I'll be focusing on
the paper, ["Taming Transformers for high-resolution images
synthesis."](https://arxiv.org/abs/2012.09841) On the other hand, if you want
to learn more about CLIP, I suggest reading [OpenAI's
explainer](https://openai.com/blog/clip/). It's comprehensive and accessible. 


<div style="border:3px; border-style:solid; border-color:#a00000; padding: 1em;">
<b>Contents</b><br>
<ul>
    <li>How we see images: a theory of perception</li>
    <li>Using Transformers to model interactions</li>
    <li>Expressing modalities through a codebook</li>
    <li>Training the codebook using a GAN</li>
    <li>Cost-function tricks</li>
</ul>
<b>Appendix</b>
<ol>
    <li>Literature leading to VQGAN</li>
    <li>Revisiting CLIP</li>
</ol>
</div>

<br>


## How we see images: a theory of perception

If I show you this picture, how would you describe it?

<!-- key takeaway: we don't think in terms of pixels, we think in terms of
modalities -->

<!-- there's also long range dependencies -->

<!-- how should we model modalities? -->

## Using Transformers to model interactions

<!-- transformers have been proven to be good at long sequences and capture
long-range dependencies, we should take advantage of that -->

<!-- However, the problem is that using transformers is not scaleable.
Everything is still in pixels -->


## Expressing modalities through a codebook 


## Training the codebook using a GAN (w/ VQ)

## Cost-function tricks


## References

1. <a id="esser2021vqgan">Esser, P., Rombach, R. and Ommer, B.</a>, 2021. Taming transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 12873-12883).
2. <a id="radford2021clip">Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G.</a>, 2021. Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*.



