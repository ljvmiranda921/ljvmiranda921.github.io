---
layout: post
type: post
title: "Chain of thought prompting for corpus annotation"
date: 2023-03-28
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [nlp, llm, data annotation, prodigy, natural language processing, chatgpt, gpt-j, gpt-3]
header-img: /assets/png/tagalog-gold-standard/header.png
description: |
    We can prompt large language models like GPT-3 to obtain zero- and few-shot
    annotations for NER and text categorization tasks. But what happens if we
    apply chain-of-thought to our prompts? What's the benefit of using this
    technique?
excerpt: |
    We can prompt large language models like GPT-3 to obtain zero- and few-shot
    annotations for NER and text categorization tasks. But what happens if we
    apply chain-of-thought to our prompts? What's the benefit of using this
    technique?
---

<span class="firstcharacter">R</span>ecently, I've been working on a project...

<!--
Few-shot vs. chain of thought (table)
- types of examples


HCI
- annotation disagreement? look for RRL here
- ...

-->