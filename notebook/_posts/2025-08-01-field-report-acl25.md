---
layout: post
type: post
title: "Field Report: ACL 2025"
date: 2025-08-01
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [nlp, acl, conference, research, natural language processing, ai, llm, reasoning]
description: |
    Here is my field report from the ACL 2025 Conference in Vienna.
    I learned a lot about X, got excited about Y, and had lots of fun all throughout!
excerpt: |
---

<span class="firstcharacter">I</span> had an incredible time at the [ACL 2025 Conference](https://2025.aclweb.org/) in Vienna.
ACL is one of the top conferences in NLP, with researchers presenting a wide range of work from computational linguistics to frontier large language models.
This was also the very first NLP conference I attended.
Although I had published in these venues before, I never had a chance to attend in person.
Attending ACL was also a great way to immerse myself in the broader NLP community before starting my Ph.D.

<!-- pictures of vienna and the place baby! -->


## Works I presented during the conference

Of course, some self-promotion: I had four papers accepted in ACL Main, three of which I was the first or co-first author on.
This seems like a big feat, but these works were completed at different times&mdash;it's just that the timing led them to be published simultaneously:

- [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)<br>TLDR: We find that some preference instances are better annotated by humans than by language models (LMs). We use that information to train a hybrid preference router (HyPER) that allocates instances to either humans or LMs.
- [M-RewardBench: Evaluating Reward Models in Multilingual Settings.](https://arxiv.org/abs/2410.15522)<br>TLDR: We introduce a new benchmark for evaluating reward models (RMs) in 23 diverse languages. By evaluating several RMs on M-RewardBench, we uncovered signficant gaps in RM performance between English and non-English languages.
- [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)<br>TLDR: We introduce the largest Tagalog treebank to date, 100x larger than previous treebanks. 
By building this treebank, we stretch the limits of the Universal Dependencies framework and challenge its "universality."

And a big collab project from SEACrowd:

- [Crowdsource, Crawl, or Generate? Creating SEA-VL, a Multicultural Vision-Language Dataset for Southeast Asia](https://arxiv.org/abs/2503.07920)<br>TLDR: We present SEA-VL, one of the largest open-source initiatives to develop high-quality and culturally-relevant data for SEA languages. We also highlight representation gaps in SEA, especially for vision-language models.

As I've said, these were all completed at different times.
In fact, HyPER was a reject from ICLR and my involvement in UD-NewsCrawl started way back in 2023 (data collection started in 2021).
Good research takes time, and I'm lucky to have great collaborators during these long periods of hard work.

## Interesting papers

<!-- top five papers -->

<!-- three honorable mentions??? -->


## NLP research vibes and trends


### Some deltas in my research beliefs



## Miscellaneous

* **I attended the SEACrowd Birds-of-a-Feather (BoF) session!** Finally, I've met my long-time collaborators.


## Final thoughts

Attending ACL 2025 is a great way to close my pre-PhD years and start my new journey as a PhD student.
As my first NLP conference, ACL gave me a good first impression of the community at large.
Perhaps I've been lucky with the people I've talked to, but everyone is warm, friendly, and welcoming.
Good vibes all throughout, no notes.

Also, I kinda like writing these field reports.
It's a good way for me to synthesize things I've learned during the conference, and hopefully it's a good way for non-attendees to get an inside look into these academic research events.
I admit I haven't been writing in this blog recently (although in general, I've written way more during the past two years), so this might be a good way to fill-in those spaces during the year.
Hoping to write more field reports in the future!