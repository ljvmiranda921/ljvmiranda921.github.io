---
layout: post
type: post
title: "Study notes on parameter-efficient finetuning techniques"
date: 2023-05-15
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [efficient nlp, nlp, peft, parameter efficient finetuning, gpt, llm, large language models]
header-img: /assets/png/langchain/header.png
description: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
excerpt: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
---

<span class="firstcharacter">F</span>inetuning is a way to adapt pretrained
language models (LMs) to a specific task or domain. It requires attaching a task
head to the model and updating the weights of the entire network. However, this
process can put a strain on one's compute budget. This becomes more true as
language models get larger and larger in every release.

In this blog post, I want to share my notes on **parameter-efficient finetuning
techniques (PEFT).** Here, we only finetune on a small number of parameters
while keeping most of the LM parameters frozen. As a result, PEFT allows domain
adaptation at a lower compute and storage costs. Lastly, this blog post is not a
literature review; I will only discuss methods I personally like. For each PEFT,
I will talk about its overview, related works, and high-level implementation.

## Finetuning is the de facto transfer learning technique, but it's inefficient

To recap, pretrained language models like BERT ([Devlin et al.,
2019](#devlin2019bert)) contain contextualized word representations that capture
the meaning of each token and its context within the text. By themselves,
they're already useful. However, language models have enjoyed greater
versatility and state-of-the-art performance because of finetuning ([Howard and
Ruder, 2018](#howard2018ulmfit)).

Much of the pretrained LMs we use today are based on transformer networks
([Vaswani et al., 2017](#vaswani2017attention)). Let's review its architecture
as it will be useful for understanding the PEFT techniques later on. Recall that
most transformer networks consist of a stack of encoder and decoder layers with
an attention mechanism:


![](/assets/png/peft/transformer_network.png){:width="700px"}  
{:style="text-align: center;"}

The encoder layer consists of two sub-layers: an attention layer and a
feedforward network. On the other hand, the decoder layer has the same two plus
a cross-attention layer added to the encoder output. Between each sub-layer,
there is a residual (or skip) connection that is normalized through LayerNorm
([Ba et al., 2016](#ba2016layernorm)). Here's what a typical encoder layer looks
like:

![](/assets/png/peft/inside_encoder.png){:width="600px"}  
{:style="text-align: center;"}


One common way to finetune is to attach a task-specific head at the end of a
pretrained language model then train the entire network on our labeled data.[^1]
However, this process is slowly becoming inefficient ([Treviso et al., 2023]()). The number of
parameters in pretrained language models has increased exponentially (Lakim et
al., 2022), exacerbating fears in an LM's environmental impact ([Strubell et al.,
2019]()) and making them inaccessible in resource constrained
environments ([Ahmed and Wahed, 2020]()) and consumer-grade hardware ([Thompson et al., 2020]()).


<!-- include steps? -->


<!-- maybe small hf pseudocode? -->



<!-- However, this process is slowly getting inefficient (citation needed) -->



[^1]:

    Note that it's also possible to freeze the entire pretrained LM and only
    update the weights of the task-specific head. However, this process only
    works if the task-specific data is small to avoid overfitting. In most
    cases, updating both set of weights leads to better performance on the task
    at hand.


<!--

- To recap, pretrained language models contain contextualized word representations that capture the meaning of each token and its context within the text.
- By themselves, they're already useful. However, language models have enjoyed greater versatility because of finetuning. 


- Pretrained language models have enjoyed greater versatility because of finetuning. 
- Context-sensitive vectors 


-->

<!--

## There are efficient ways to adapt pretrained models


### Adapters - 

### Prompt tuning

### LoRA

-->

<!-- adapter networks -->


<!-- prefix tuning -->


<!-- mixture of experts -->

## References

- <a id="howard2018ulmfit">Jeremy Howard and Sebastian Ruder</a>. 2018. Universal Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 328–339, Melbourne, Australia. Association for Computational Linguistics.
- <a id="devlin2019bert">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</a>. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- <a id="ba2016layernorm">Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton</a>. "Layer normalization." arXiv preprint arXiv:1607.06450 (2016).
- <a id="vaswani2017attention">Vaswani, Ashish, et al</a>. "Attention is all you need." *Advances in neural information processing systems 30* (2017).


### Footnotes