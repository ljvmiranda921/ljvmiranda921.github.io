---
layout: post
type: post
title: "Study notes on parameter-efficient finetuning techniques"
date: 2023-05-15
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [efficient nlp, nlp, peft, parameter efficient finetuning, gpt, llm, large language models]
header-img: /assets/png/langchain/header.png
description: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
excerpt: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
---

<span class="firstcharacter">F</span>inetuning is a way to adapt pretrained
language models (LMs) to a specific task or domain. It requires attaching a task
head to the model and updating the weights of the entire network. However, this
process can put a strain on one's compute budget. This becomes more true as
language models get larger and larger in every release.

In this blog post, I want to share my notes on **parameter-efficient finetuning
techniques (PEFT).** Here, we only finetune on a small number of parameters
while keeping most of the LM parameters frozen. As a result, PEFT allows domain
adaptation at a lower compute (and storage) cost. Lastly, this blog post is not a
literature review; I will only discuss methods I personally like. For each PEFT,
I will talk about its overview, related works, and high-level implementation.

## Finetuning is the de facto transfer learning technique, but it has become inefficient

To recap, pretrained language models like BERT ([Devlin et al.,
2019](#devlin2019bert)) contain contextualized word representations that capture
the meaning of each token and its context within the text. By themselves,
they're already useful. However, language models have enjoyed greater
versatility and state-of-the-art performance because of finetuning ([Howard and
Ruder, 2018](#howard2018ulmfit)).

Much of the pretrained LMs we use today are based on transformer networks
([Vaswani et al., 2017](#vaswani2017attention)). Let's review its architecture
as it will be useful for understanding the PEFT techniques later on. Recall that
most transformer networks consist of a stack of encoder and decoder layers with
an attention mechanism:


![](/assets/png/peft/transformer_network.png){:width="700px"}  
{:style="text-align: center;"}

The encoder layer consists of two sub-layers: an attention layer and a
feedforward network. Its outputs are passed to the decoder, consisting of the
same two sub-layers plus a cross-attention mechanism that attends the encoder's
output. Between each sub-layer, there is a residual (or skip) connection that is
normalized through LayerNorm ([Ba et al., 2016](#ba2016layernorm)). 

For transformers like BERT, there is no generative step, hence it only contains
encoders. Here's what a typical encoder layer looks like:

![](/assets/png/peft/inside_encoder.png){:width="600px"}  
{:style="text-align: center;"}


```python
# Pseudocode of a typical encoder layer
class Encoder:
    def __call__(self, x: Tensor) -> Tensor:
        residual = x
        x = MultiHeadAttention(x)
        x = LayerNorm(x + residual)
        residual = x
        x = FeedForwardLayers(x)
        x = LayerNorm(x + residual)
        return x
```

One common way to finetune is to attach a task-specific head at the end of a
pretrained language model then train the entire network on our labeled data.[^1]
Although these LMs were trained on different tasks (for example, BERT was
trained on masked language modeling and next sentence prediction), it is
possible to refine its weights for other NLP problems (e.g., sentence tagging,
sequence classification, etc.).

![](/assets/png/peft/finetuning.png){:width="700px"}  
{:style="text-align: center;"}


However, this process has become inefficient ([Treviso et al.,
2023](#treviso2023efficient)). The number of parameters in pretrained language
models has increased exponentially, exacerbating fears in an LM's environmental
impact ([Strubell et al., 2019](#strubell2019energy) and making them
inaccessible in resource-constrained and consumer-grade environments.  Hence, an
efficient approach to finetuning is becoming more desirable.



[^1]:

    Note that it's also possible to freeze the entire pretrained LM and only
    update the weights of the task-specific head. However, this process only
    works if the task-specific data is small to avoid overfitting. In most
    cases, updating both set of weights leads to better performance on the task
    at hand.

## We can make the finetuning process more efficient and modular

Recently, I've been interested in parameter-efficient techniques (PEFT) that are
modular in nature. Most of these involve creating small, trainable modules
(sometimes a feedforward network or a special matrix) that we attach to
different parts of a larger, pretrained network. Usually, we keep the larger
network frozen while we train the smaller modules on our data. 

> Most [parameter-efficient techniques] involve creating small, trainable modules
> that we attach to different parts of a larger, pretrained network.

**I like PEFT because it appeals to my engineering sensibilities.** I like the
["separation of concerns"](https://en.wikipedia.org/wiki/Separation_of_concerns)
between our network's fundamental understanding about language (pretrained
network) and its task-specific capabilities (smaller modules). We can even
aggregate these modules to solve multi-domain or multilingual problems
([Gurugangan et al., 2022](#gurugangan2022demix); [Chronopoulou et al.,
2023](#chronopoulou2023adaptersoup); [Asai et al., 2022](#asai2022attempt)).

<!--

"separation of concerns"
"hierarchy"

-->



<!-- don't spend much time on this, maybe introduce modular NLP and efficient NLP lit reviews? -->


<!--

### Adapters: attach small, trainable modules between transformer layers

#### High-level implementation

```python
# Pseudocode of a typical encoder layer with adapter
class EncoderWithAdapter:
    def __call__(self, x: Tensor) -> Tensor:
        residual = x
        x = MultiHeadAttention(x)
        x = AdapterNetwork(x)  # Usually another feedforward layer
        x = LayerNorm(x + residual)
        residual = x
        x = FeedForwardLayers(x)
        x = AdapterNetwork(x)  # Usually another feedforward layer
        x = LayerNorm(x + residual)
        return x
```

```python
class AdapterNetwork:
    def __call__(self, x: Tensor) -> Tensor:
        pass

```

#### In practice


-->



<!--

## There are efficient ways to adapt pretrained models


### Adapters - 

### Prompt tuning

### LoRA

-->

<!-- adapter networks -->


<!-- prefix tuning -->


<!-- mixture of experts -->

## References

- <a id="howard2018ulmfit">Jeremy Howard and Sebastian Ruder</a>. 2018. Universal Language Model Fine-tuning for Text Classification. In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 328–339, Melbourne, Australia. Association for Computational Linguistics.
- <a id="devlin2019bert">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova</a>. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
- <a id="ba2016layernorm">Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton</a>. "Layer normalization." arXiv preprint arXiv:1607.06450 (2016).
- <a id="vaswani2017attention">Vaswani, Ashish, et al</a>. "Attention is all you need." *Advances in neural information processing systems 30* (2017).
- <a id="treviso2023efficient">Marcos Treviso, et al.</a> "Efficient methods for natural language processing: a survey." arXiv preprint arXiv:2209.00099 (2022).
- <a id="strubell2019energy">Emma Strubell, Ananya Ganesh, and Andrew McCallum.</a> "Energy and policy considerations for deep learning in NLP." arXiv preprint arXiv:1906.02243 (2019).
- <a id="gurugangan2022demix">Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer</a>. 2022. DEMix Layers: Disentangling Domains for Modular Language Modeling. In *Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 5557–5576, Seattle, United States. Association for Computational Linguistics.
- <a id="chronopoulou2023adaptersoup">Alexandra Chronopoulou</a>, et al. "Adaptersoup: Weight averaging to improve generalization of pretrained language
models." arXiv preprint arXiv:2302.07027 (2023).
- <a id="asai2022attempt">Akari Asai, Mohammadreza Salehi, Matthew Peters, and Hannaneh Hajishirzi</a>. 2022. ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 6655–6672, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.



### Footnotes