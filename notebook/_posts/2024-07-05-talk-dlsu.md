---
layout: post
type: post
title: "Guest lecture @ DLSU Manila: Artisanal Filipino NLP Resources in the time of Large Language Models"
date: 2024-07-05
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
header-img: /assets/png/talk-unc-charlotte/header.png
tags:
  [
    argument mining,
    fake news,
    llm,
    evaluation,
    annotation,
    prodigy,
    ai,
    large language models,
  ]
description: |
  Last month, I had another guest lecture, this time in Dr. Charibeth Cheng's graduate class in DLSU! 
  Here, I talked about the craft of building small-scale yet effective NLP models for Filipino in the face of today's large language models.
excerpt: |
---

<span class="firstcharacter">I</span> was invited to give a talk to a graduate-level NLP class about my work on Filipino resources.
It was fun preparing and giving that talk because I was able to synthesize my thoughts and look back on my previous research.
This blog post is my lecture in text format. 
**You can find the slides in this [link](https://docs.google.com/presentation/d/10wrKZoBouh3agrgkTLjvJN9g9WwqdrjiwWSSztoyWdA/edit?usp=sharing).**
Finally, thank you to [Dr. Charibeth Cheng](https://www.dlsu.edu.ph/colleges/ccs/faculty-profile/cheng-charibeth/) for the invitation!

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTRVq0Lo3adiVDkcgJpsuz0RHDFvFWS-9gj2r2kg2dIi-33BvnRSYH1FyOiUQ0dSys_sT44f7uyHigz/embed?start=false&loop=true&delayms=3000" frameborder="0" width="720" height="434" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

&nbsp;

---

&nbsp;

Given all the rage in LLMs today, is it still worth it to build artisanal Filipino NLP resources?
Hopefully we can answer this question in the context of the work that I've done.
I've worked on large models in the 7B-70B parameter range, but I've also done several things for low-resource languages and built models in the ~100M parameter range.
Tonight, I want to juxtapose these two sides and call one as *artisanal* and the other as *large-scale*.

## What is artisanal NLP?

In this talk, I want to contrast two types of ideas when building language technologies.
You have *artisanal* on one end, as shown by this photo of a handmade pottery&mdash;carefully constructed by its creator.
On the other hand, you have these "mass-produced" objects done in *large-scale*.
I want to differentiate them in terms of three dimensions.

![](/assets/png/talk-dlsu/slide00.png){:width="720px"}
{: style="text-align: center;"}

1. **Effort**: artisanal NLP resources often require specialized knowledge and effort. 
For example, it is important to know something about Filipino grammar and morphology when building task-specific models for Filipino.
Large-scale models can get away with just providing large amounts of data (as we see in most web-scale datasets). 

2. **Size**: currently, our definitions of what's small or large change every month.
But in general, artisanal models are relatively smaller in terms of parameter size.
Large-scale models need to be bigger because they have to accommodate more tasks and domains.

3. **Utility**: artisanal models and datasets tend to be bespoke, i.e., built for a specific task or requirement.
On the other hand, most large-scale models we see today were made for general-purpose applications.

Notice that I'm a bit vague whether I'm talking about models or datasets.
I like to think of artisanal vs. large-scale as an *attitude* for building language technologies or NLP artifacts.
Finally, this talk is not about one being better than the other. 
However, I want to focus more on the merits of these artisanal NLP resources by discussing parts of my research and my work.

You can see the outline of my lecture below.
For the rest of this talk, I'll fill the blanks and talk about the merits of artisanal NLP while discussing portions of my research.

![](/assets/png/talk-dlsu/slide01.png){:width="700px"}
{: style="text-align: center;"}


## High-effort, but impactful for the language community

In this section, I want to talk about [TLUnified-NER](https://huggingface.co/datasets/ljvmiranda921/tlunified-ner) (link to [SEALP '23 paper](https://aclanthology.org/2023.sealp-1.2/)), a Named-Entity Recognition dataset that I've built.
NER is a classical NLP problem: given a text, you want to look for named-entities such as names of persons, locations, or organizations.

![](/assets/png/talk-dlsu/slide02.png){:width="700px"}
{: style="text-align: center;"}

This is already an easy task for English NER. 
However, NER resources for Tagalog are still lacking.
We don't have a lot of labeled data, and in consequence we don't have a lot of models.
There are many ways to get around this problem (e.g., cross-lingual transfer learning, zero-shot from an LLM),
but we still lack reliable test sets for evaluation.

![](/assets/png/talk-dlsu/slide03.png){:width="700px"}
{: style="text-align: center;"}

In my opinion, a good NER dataset should be open-access, high-quality, and standardized.
Most of the NER datasets available for us only fills two of these three attributes:
[WikiANN](https://huggingface.co/datasets/unimelb-nlp/wikiann) has general-purpose tags that follow CoNLL and can be downloaded from HuggingFace, but the [quality of annotations are pretty bad](https://arxiv.org/pdf/2202.12288).
[LORELEI](https://catalog.ldc.upenn.edu/LDC2023T02) is a high-quality dataset, but has strict license restrictions and quite expensive!
Finally, we have several hand-annotated datasets for Filipino, but most of them were made for highly-specific tasks.
Back in 2022, there's an obvious gap to fill for Tagalog NER.

![](/assets/png/talk-dlsu/slide04.png){:width="360px"}
![](/assets/png/talk-dlsu/slide05.png){:width="360px"}
![](/assets/png/talk-dlsu/slide06.png){:width="360px"}
![](/assets/png/talk-dlsu/slide07.png){:width="360px"}
{: style="text-align: center;"}

And so we built TLUnified-NER.
It is publicly accessible, high-quality, and follows the CoNLL standard.
I also curated the texts to ensure that it represents how we commonly write Tagalog.
The annotation process is done through several rounds (or sprints).
I hired two more annotators and then for each round we annotate a batch of examples, evaluate the annotated batch, and update the annotation guidelines to improve quality.
You can learn more about this process in [**our paper**](https://aclanthology.org/2023.sealp-1.2.pdf).
I also wrote some of my thoughts on the annotation process [in a blogpost](/notebook/2023/07/03/devlog-calamancy/).

After building the dataset, there are two questions that I want to answer:
first, is the NER task learnable from our annotations? Then, is it better than existing NER datasets?
For the first one, I created baseline approaches using various mixes of word embeddings and language coverage.
For all cases, we achieved decent performance.
Then for the second question, we compared a model trained on WikiANN and a model trained from TLUnified-NER.
In most cases, our model outperforms the WikiANN model, showing that it's a better dataset to train models upon.

![](/assets/png/talk-dlsu/slide07.png){:width="360px"}
![](/assets/png/talk-dlsu/slide08.png){:width="360px"}
{: style="text-align: center;"}

