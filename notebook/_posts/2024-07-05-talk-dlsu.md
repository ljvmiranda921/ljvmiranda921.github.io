---
layout: post
type: post
title: "Guest lecture @ DLSU Manila: Artisanal Filipino NLP Resources in the time of Large Language Models"
date: 2024-07-05
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
header-img: /assets/png/talk-unc-charlotte/header.png
tags:
  [
    argument mining,
    fake news,
    llm,
    evaluation,
    annotation,
    prodigy,
    ai,
    large language models,
  ]
description: |
  Last month, I had another guest lecture, this time in Dr. Charibeth Cheng's graduate class in DLSU. 
  Here, I talked about the craft of building small-scale yet effective NLP models for Filipino in the face of today's large language models.
excerpt: |
---

<span class="firstcharacter">I</span> was invited to give a talk to a graduate-level NLP class about my work on Filipino resources.
It was fun preparing and giving that talk because I was able to synthesize my thoughts and look back on my previous research.
This blog post is my lecture in text format. 
**You can find the slides in this [link](https://docs.google.com/presentation/d/10wrKZoBouh3agrgkTLjvJN9g9WwqdrjiwWSSztoyWdA/edit?usp=sharing).**
Finally, thank you to [Dr. Charibeth Cheng](https://www.dlsu.edu.ph/colleges/ccs/faculty-profile/cheng-charibeth/) for the invitation!

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vTRVq0Lo3adiVDkcgJpsuz0RHDFvFWS-9gj2r2kg2dIi-33BvnRSYH1FyOiUQ0dSys_sT44f7uyHigz/embed?start=false&loop=true&delayms=3000" frameborder="0" width="720" height="434" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>

&nbsp;

---

&nbsp;

Given all the rage in LLMs today, is it still worth it to build artisanal Filipino NLP resources?
Hopefully we can answer this question in the context of the work that I've done.
I've worked on large models in the 7B-70B parameter range, but I've also done several things for low-resource languages and built models in the ~100M parameter range.
Tonight, I want to juxtapose these two sides and call one as *artisanal* and the other as *large-scale*.

## What is artisanal NLP?

In this talk, I want to contrast two types of ideas when building language technologies.
You have *artisanal* on one end, as shown by this photo of a handmade pottery&mdash;carefully constructed by its creator.
On the other hand, you have these "mass-produced" objects done in *large-scale*.
I want to differentiate them in terms of three dimensions.

![](/assets/png/talk-dlsu/slide00.png){:width="720px"}
{: style="text-align: center;"}

1. **Effort**: artisanal NLP resources often require specialized knowledge and effort. 
For example, it is important to know something about Filipino grammar and morphology when building task-specific models for Filipino.
Large-scale models can get away with just providing large amounts of data (as we see in most web-scale datasets). 

2. **Size**: currently, our definitions of what's small or large change every month.
But in general, artisanal models are relatively smaller in terms of parameter size.
Large-scale models need to be bigger because they have to accommodate more tasks and domains.

3. **Utility**: artisanal models and datasets tend to be bespoke, i.e., built for a specific task or requirement.
On the other hand, most large-scale models we see today were made for general-purpose applications.

Notice that I'm a bit vague whether I'm talking about models or datasets.
I like to think of artisanal vs. large-scale as an *attitude* for building language technologies or NLP artifacts.
Finally, this talk is not about one being better than the other. 
However, I want to focus more on the merits of these artisanal NLP resources by discussing parts of my research and my work.

You can see the outline of my lecture below.
For the rest of this talk, I'll fill the blanks and talk about the merits of artisanal NLP while discussing portions of my research.

![](/assets/png/talk-dlsu/slide01.png){:width="700px"}
{: style="text-align: center;"}


## Artisanal NLP resources are high-effort, but impactful for the language community

In this section, I want to talk about [TLUnified-NER](https://huggingface.co/datasets/ljvmiranda921/tlunified-ner) (link to [SEALP '23 paper](https://aclanthology.org/2023.sealp-1.2/)), a Named-Entity Recognition dataset that I've built.
NER is a classical NLP problem: given a text, you want to look for named-entities such as names of persons, locations, or organizations.

![](/assets/png/talk-dlsu/slide02.png){:width="700px"}
{: style="text-align: center;"}

This is already an easy task for English NER. 
However, NER resources for Tagalog are still lacking.
We don't have a lot of labeled data, and in consequence we don't have a lot of models.
There are many ways to get around this problem (e.g., cross-lingual transfer learning, zero-shot from an LLM),
but we still lack reliable test sets for evaluation.

![](/assets/png/talk-dlsu/slide03.png){:width="700px"}
{: style="text-align: center;"}

In my opinion, a good NER dataset should be open-access, high-quality, and standardized.
Most of the NER datasets available for us only fills two of these three attributes:
[WikiANN](https://huggingface.co/datasets/unimelb-nlp/wikiann) has general-purpose tags that follow CoNLL and can be downloaded from HuggingFace, but the [quality of annotations are pretty bad](https://arxiv.org/pdf/2202.12288).
[LORELEI](https://catalog.ldc.upenn.edu/LDC2023T02) is a high-quality dataset, but has strict license restrictions and quite expensive!
Finally, we have several hand-annotated datasets for Filipino, but most of them were made for highly-specific tasks.
Back in 2022, there's an obvious gap to fill for Tagalog NER.

![](/assets/png/talk-dlsu/slide04.png){:width="360px"}
![](/assets/png/talk-dlsu/slide05.png){:width="360px"}
![](/assets/png/talk-dlsu/slide06.png){:width="360px"}
![](/assets/png/talk-dlsu/slide07.png){:width="360px"}
{: style="text-align: center;"}

And so we built TLUnified-NER.
It is publicly accessible, high-quality, and follows the CoNLL standard.
I also curated the texts to ensure that it represents how we commonly write Tagalog.
The annotation process is done through several rounds (or sprints).
I hired two more annotators and then for each round we annotate a batch of examples, evaluate the annotated batch, and update the annotation guidelines to improve quality.
You can learn more about this process in [**our paper**](https://aclanthology.org/2023.sealp-1.2.pdf).
I also wrote some of my thoughts on the annotation process [in a blogpost](/notebook/2023/07/03/devlog-calamancy/).

After building the dataset, there are two questions that I want to answer:
first, is the NER task learnable from our annotations? Then, is it better than existing NER datasets?
For the first one, I created baseline approaches using various mixes of word embeddings and language coverage.
For all cases, we achieved decent performance.
Then for the second question, we compared a model trained on WikiANN and a model trained from TLUnified-NER.
In most cases, our model outperforms the WikiANN model, showing that it's a better dataset to train models upon.

![](/assets/png/talk-dlsu/slide07.png){:width="360px"}
![](/assets/png/talk-dlsu/slide08.png){:width="360px"}
{: style="text-align: center;"}

To end this part of the talk, I want to show that **NER is just one piece of the NLP puzzle.**
There are still a lot of tasks to build resources on.
I believe that increasing the coverage of Filipino resources allows us to not only train models, but create comprehensive evaluation benchmarks for existing LLMs today.
Recently, we've seen a lot of claims that LLMs can "speak" Filipino, but most of these are cherry-picked examples and highly vibes-based.
If we can create a systematic benchmark that allows us to confidently claim performance, then that would be a big contribution to the field.

![](/assets/png/talk-dlsu/slide09.png){:width="700px"}
{: style="text-align: center;"}

## Artisanal NLP resources are capable of doing a few things, but can do them well

In this section, I'll talk about [calamanCy](https://github.com/ljvmiranda921/calamanCy), a spaCy-based toolkit that I built for Tagalog ([NLP-OSS '23 paper](https://aclanthology.org/2023.nlposs-1.1/)).
As you already know, spaCy is a toolkit for core linguistic tasks such as dependency parsing, tokenization, and NER.
However, most of the models we provide in-house are focused on high-resource languages.

What most people do is they finetune spaCy pipelines on their own language or domain.
So you'll see libraries in the [spaCy universe](https://spacy.io/universe) for all kinds of applications.

<!-- - Multilinguality: [DaCy](https://github.com/centre-for-humanities-computing/DaCy) for Danish and [HuSpaCy](https://github.com/huspacy/huspacy) for Hungarian.
- Domain-specific texts: [Hobbit-spaCy](https://github.com/wjbmattingly/hobbit-spacy), [scispaCy](https://github.com/allenai/scispacy), and [medspaCy](https://github.com/medspacy/medspacy) for fictional, scientific, and medical texts.
- Ancient languages: [latinCy](https://huggingface.co/latincy) for Latin, [greCy](https://github.com/jmyerston/greCy) for Greek, and [**my work on SIGTYP '24**](https://aclanthology.org/2024.sigtyp-1.18/) on several Ancient & Medieval languages. -->

![](/assets/png/talk-dlsu/slide10.png){:width="700px"}
{: style="text-align: center;"}

First, think of a [spaCy pipeline](https://spacy.io/usage/processing-pipelines) as a series of functions that identifies key linguistic features in a text.
So a tokenizer is a function that looks for tokens, a tagger is a function for parts-of-speech (POS) tags, and so on.
Then at the end, you obtain a [`Doc` object](https://spacy.io/api/doc) that contains all these linguistic features.

![](/assets/png/talk-dlsu/slide11.png){:width="700px"}
{: style="text-align: center;"}

One of the hardest parts of building [calamanCy](https://github.com/ljvmiranda921/calamanCy) is curating datasets to "train" these functions. 
For example, the current Tagalog treebanks are [too small to train a reliable dependency parser and POS tagger](/notebook/2022/04/24/low-resource-dep-parse/).
Also, TLUnified-NER doesn't exist back then, so I still have to build it.
You can read more about this curation process in the [calamanCy paper](https://aclanthology.org/2023.nlposs-1.1/).

![](/assets/png/talk-dlsu/slide12.png){:width="700px"}
{: style="text-align: center;"}

It was a long process, but the most important question is: was it worth it?
To that I remember this figure from Matthew Honnibal's blog post on [LLM maximalism](https://explosion.ai/blog/against-llm-maximalism).
I think there is value in curating these datasets and training these models as it helps me understand which parts of the linguistic pipeline really requires an LLM, and which could be done more reliably by a simple approach.
In addition, we were also able to show empirically that models trained on calamanCy performs pretty well, even compared to commercial LLM APIs.

![](/assets/png/talk-dlsu/slide13.png){:width="700px"}
{: style="text-align: center;"}


![](/assets/png/talk-dlsu/slide14.png){:width="360px"}
![](/assets/png/talk-dlsu/slide15.png){:width="360px"}
{: style="text-align: center;"}


![](/assets/png/talk-dlsu/slide16.png){:width="700px"}
{: style="text-align: center;"}