---
layout: post
type: post
title: "Your train-test split may be doing you a disservice"
date: 2022-08-30
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [machine learning, adversarial splitting, deep learning, natural language processing, nlp]
description: |
    Have you ever encountered a problem where your model works well in your test
    set but doesn't perform well in the wild? It's likely because your test set
    does not reflect the reality of your domain, overestimating our model's
    performance. In this blog post, I'll discuss alternative ways to split data
    and examine their effect on named-entity recognition (NER) benchmarks. 
excerpt: |
    Have you ever encountered a problem where your model works well in your test
    set but doesn't perform well in the wild? It's likely because your test set
    does not reflect the reality of your domain, overestimating our model's
    performance. In this blog post, I'll discuss alternative ways to split data
    and examine their effect on named-entity recognition (NER) benchmarks. 
---

<span class="firstcharacter">F</span>irst, let's recap a typical data science
workflow: every project starts by splitting our data into train, development
(dev), and test partitions. Then we fit our model using the train and dev sets
and compute the score using a held-out test set. 

Most of the time, we shuffle our data before splitting, creating **random
splits**. For some benchmark datasets like
[MNIST](http://yann.lecun.com/exdb/mnist/) or
[OntoNote 5.0](https://catalog.ldc.upenn.edu/LDC2013T19), these partitions are
already included in the task, providing us with **standard splits**. 

However, it turns out that random and standard splits can lead to **test sets
that are not wholly representative of the actual domain** ([Gorman and Bedrick,
2019](#gorman2019standard), and [Søgaard et. al., 2021](#sogaard2021random)).
It's also possible that the training and test sets look very similar,
encouraging the model to **memorize the training set without actually learning
anything** ([Goldman et al., 2022](#goldman2022morph)). This behavior results in overestimated performances that don't
translate well into production.

> It turns out that random and standard splits can lead to test sets that are
> not wholly representative of the actual domain...resulting into overestimated
> performance that don't translate well into production.

This blog post discusses **alternative ways to split our datasets to provide
more realistic performance.** I will call all methods under this umbrella
**alternative splits**. I'll also investigate **how alternative splits
affect model performance on the named-entity recognition (NER) task** using
[spaCy's transition-based NER](https://spacy.io/api/entityrecognizer) and the
WikiNeural, WNUT17, and ConLL-2003 datasets.

| Dataset              | Source                                                                                                             | Description                                                                                                                                          |
|----------------------|--------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|
| WikiNEuRal (English) | WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER (Tedeschi, et al., 2021) | Datset based from Wikipedia that takes advantage of knowledge-based approaches and neural models to create a silver dataset.                         |
| WNUT17               | Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition (Derczynski, et al., 2017)            | Dataset consisting of unusual, previously-unseen entities in the context of emerging discussions.                                                    |
| CoNLL 2003           | Introduction to the CoNLL-2003: Shared Task: Language-Independent Named Entity Recognition (Sang et al., 2003)     | Standard NER Benchmark dataset where the English data was taken from the Reuters Corpus. It consists of stories between August 1996 and August 1997. |

> I want to take this opportunity to introduce a Python package that
> I'm currently working on, ⚔[️ **`vs-split`**](https://github.com/ljvmiranda921/vs-split). It's still a work-in-progress, but it implements some techniques I'll mention throughout the post.

## Splitting by maximizing divergence

A central assumption in machine learning is that the **training and test sets
are from the same distribution**, i.e., they are [independent and identically
distributed
(i.i.d.)](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) ([Gilmer, 2020](#gilmer2020robustness)).
But let's drop that for a moment. We know that the i.i.d. assumption can lead to
grossly overestimated model performances, but **what if we evaluate the model
with the "worst" possible test set?**

We can obtain the "worst" test set by **ensuring that the train and test sets
have different distributions**, i.e., they're wholly divergent. In literature,
this mode of splitting is called **adversarial splits**. We can measure this
value using a metric called the [Wasserstein
distance](https://en.wikipedia.org/wiki/Wasserstein_metric). However, finding
the right combination of train and test examples that maximizes this metric is
an NP-hard problem, so [Sogaaard et al.  (2021)](#sogaard2021random) used an
approximate approach involving k-nearest neighbors with a ball-tree algorithm.

If we run this method through the three datasets above, we can observe a short
drop in test performance (measured by the Precision / Recall / F-score, lower
values are **highlighted**):


| Dataset              | Standard Split (P/R/F) | Wasserstein Split (P/R/F) |
|----------------------|------------------------|---------------------------|
| WikiNEuRal (English) | 0.87 / 0.86 / 0.87     |  **0.82 / 0.79  / 0.81**     |
| WNUT17               | 0.47 / 0.45 / 0.46     | **0.41 / 0.25 / 0.31**    |
| CoNLL 2003 (English) | 0.86 / 0.86 / 0.86     | **0.87 / 0.84 / 0.85**    |

Below you'll find the [displaCy output](https://spacy.io/usage/visualizers) from
ConLL 2003's adversarial test split. I admit that I *cherry-picked* some of
these examples, but one apparent pattern I saw is the presence of numbers
(currencies, dates, scores) in the adversarial test set. One possible explanation is
that the k-nearest neighbors search [uses token counts as its
feature](https://github.com/google-research/google-research/blob/master/talk_about_random_splits/probing/probing_utils.py#L152),
dividing the dataset based on token frequency.

<!--
# Template
style="padding: 20px; line-height: 2.5; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; font-size: 18px">
-->

**Example 1:** A text with unusual formatting. The way the text laid out the scores in this
sports news article made it a bit incomprehensible.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 56.25%">
<iframe src="/assets/png/splits/wasserstein-sample-00.html" height="300" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>

**Example 2:** Financial text with a lot of currencies and dates. The WikiNEuRal dataset only
has `PER`, `ORG`, `LOC`, and `MISC` as its entity types.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 56.25%">
<iframe src="/assets/png/splits/wasserstein-sample-01.html" height="300" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>

**Example 3:** Similar to the previous example, with dates, currencies, and other numerical
text.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 56.25%">
<iframe src="/assets/png/splits/wasserstein-sample-02.html" height="300" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>

## Splitting by heuristic

This section will discuss how we can **split datasets using various heuristics**
such as a document's *length* or *morphological features*.  Unlike the previous
approach, where we treat the splitting process as an optimization problem,
"splitting by heuristic" takes advantage of domain-expertise to create an
alternative test set.

For example, *document length* can be a valuable feature to gauge how well a
machine learning model adapts to cross-sentence information in NER. Longer sentences
can provide more context as to what type of entity a particular token is.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 29.25%">
<iframe src="/assets/png/splits/context_example.html" height="100" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>

In the example above, we know that the token `McKinley` can refer to either a
location (`LOC`) or a person (`PER`). Because of the context sentence, *"...was
named after William McKinley"*, the model had a better idea of what type of
entity the second instance of `McKinley` is. The works of [Luoma and Pyysalo
(2020)](#luoma2020cross), [Arora et al. (2021)](#arora2021typed), and
[Siegelmann and Sontag (1992)](#siegelmann1992nets) explores how well NER models
can take advantage of such cross-sentence information.

In our case, **we'll split the dataset so that the longer sentences are in the
test split.** Similar to [Sogaard et al's (2021)](#sogaard2021random) approach,
we choose a threshold so that $$10\%$$ of the documents will be found in the
test set. After training a transition-based NER on this new split, the results
are as follows (lower values are **highlighted**):

| Dataset              | Threshold | Standard Split (P/R/F) | Split by Length (P/R/F)   |
|----------------------|-----------|------------------------|---------------------------|
| WikiNEuRal (English) |    38 | 0.87 / 0.86  / 0.87      |  **0.84 / 0.85  / 0.85**     |
| WNUT17               |    29     | **0.47** / 0.45 / **0.46**     | 0.58 / **0.42** / 0.49    |
| CoNLL 2003 (English) |    446    | 0.86 / 0.86 / 0.86     | **0.78 / 0.84 / 0.81**    |

It's interesting why there's not much change in the WNUT17 score. We can even
say that the split improved the performance! I hypothesize that the token-length
difference between the training and test is not that apparent given the
distribution. This means that the split may not be as biased as we want it to be
and just looks like a random split.

<!-- plot distribution of lengths -->
![](/assets/png/splits/wnut17.png){:width="600px", style="padding:10px"}  
{:style="text-align: center;"}

Here are some examples from the new WNUT17 test split:
<!-- show samples from WNUT17 -->



**Example 1:** A text of token length 105. This document is one of the top 5 longest texts in the WNUT17 dataset.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 50.25%">
<iframe src="/assets/png/splits/doclength-sample-01.html" height="100" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>


**Example 2:** A text of token length 72. There seems to be an error in
*annotation. The text looks like a Tweet, but the token `@Xantec` was mislabeled
*as a `product` instead of a `person`.

<div style="position:relative; overflow: hidden; width;100%; padding-top: 48.25%">
<iframe src="/assets/png/splits/doclength-sample-02.html" height="100" width="720" style="border:1px solid #ddd;border-radius:10px;position:absolute;top:0;left:0;bottom:0;right:0;width:100%;height:100%"></iframe>
</div>
<br/>



<!-- morphs: talk about inflections for quite a bit -->


## Splitting by perturbation

<!-- talk about data augmentation, this is an offshoot of that -->

## Final thoughts

<!-- recap -->
<!-- i find three possible avenues to solve this problem (from hardest to easiest)


IDEALISTIC <-> PRACTICAL


- rebuild the whole ML ecosystem
- create better benchmarks
- add systems for check and balances
-->


## References

- <a id="sogaard2021random">Anders Søgaard, Sebastian Ebert, Jasmijn Bastings, and Katja Filippova.</a> 2021. We Need To Talk About Random Splits. In *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume*, pages 1823–1832, Online. Association for Computational Linguistics.
- <a id="gorman2019standard">Kyle Gorman and Steven Bedrick.</a> 2019. We Need to Talk about Standard Splits. In *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, pages 2786–2791, Florence, Italy. Association for Computational Linguistics.
- <a id="goldman2022morph">Omer Goldman, David Guriel, and Reut Tsarfaty.</a> 2022. (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models’ Performance. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, pages 864–870, Dublin, Ireland. Association for Computational Linguistics.
- <a id="gilmer2020robustness">Justin Gilmer.</a> The Robustness Problem. Presentation slides at [http://isl.stanford.edu/talks/talks/2020q1/justin-gilmer/slides.pdf](http://isl.stanford.edu/talks/talks/2020q1/justin-gilmer/slides.pdf)
- <a id="goot2021splits">Rob van der Goot. 2021</a>. We Need to Talk About train-dev-test Splits. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 4485–4494, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
- <a id="luoma2020cross">Jouni Luoma and Sampo Pyysalo</a>. 2020. Exploring Cross-sentence Contexts for Named Entity Recognition with BERT. *In Proceedings of the 28th International Conference on Computational Linguistics*, pages 904–914, Barcelona, Spain (Online). International Committee on Computational Linguistics.
- <a id="arora2021typed">Ravneet Arora, Chen-Tse Tsai, and Daniel Preotiuc-Pietro</a>. 2021. Identifying Named Entities as they are Typed. In *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume*, pages 976–988, Online. Association for Computational Linguistics.
- <a id="siegelmann1992nets">Hava T. Siegelmann and Eduardo D. Sontag</a>. 1992. On
the computational power of neural nets. In *Proceedings of the Fifth Annual ACM Conference on Computational Learning Theory, COLT 1992*, Pittsburgh,
PA, USA, July 27-29, 1992, pages 440–449. ACM.


