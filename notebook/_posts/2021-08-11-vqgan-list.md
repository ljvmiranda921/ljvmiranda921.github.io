---
layout: post
type: post
title: "List of VQGAN+CLIP Implementations"
date: 2021-08-11
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [vqgan, machine learning, research, deep learning, neural network, clip vqgan]
header-img: /assets/png/vqgan/ai_art_collage.png
description: |
    Here's a list of all VQGAN implementations I found in the internet.
excerpt: |
    Here's a list of all VQGAN implementations I found in the internet.
---

I've been in a VQGAN+CLIP craze lately, so here's a list of all VQGAN+CLIP
implementations I found on the internet (The symbol ðŸ”° means perfect for
non-programmers alike. If you don't know where to start, you can start with
these):

## VQGAN+CLIP implementations

| Name                                                                                                                                          | Author                                              | Description / Features                                                  |
|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------------------------|
| [VQGAN+CLIP (codebook sampling method)](https://colab.research.google.com/drive/15UwYDsnNeldJFHJ9NdgYBYeo6xPmSelP)                                 | [@RiversHaveWings](https://twitter.com/RiversHaveWings) | The original VQGAN+CLIP notebook of Katherine Crowson ([@RiversHaveWings](https://twitter.com/RiversHaveWings)).        |
| [AI Art Machine](https://colab.research.google.com/drive/1n_xrgKDlGQcCF6O-eL3NOd_x4NSqAUjK) | [@hillelogram](https://twitter.com/hillelogram)         | ðŸ”° Very accessible Colab notebook. Has advanced options that are explained in a beginner-friendly level. |
| [Create realistic AI-Generated Images with VQGAN+CLIP](https://colab.research.google.com/drive/1wkF67ThUz37T2_oPIuSwuO4e_-0vjaLs?usp=sharing) | [@minimaxir](https://twitter.com/minimaxir)         | ðŸ”° Has good UI affordances and more descriptive explanation of parameters. Have options for deterministic output by using icon-based input/target images. |
| [VQGAN+CLIP (with pooling and quantize method)](https://colab.research.google.com/drive/1Foi0mCSE6NrW9oI3Fhni7158Krz4ZXdH)                    | [@ak92501](https://twitter.com/ak92501)             | Has an optional Gradio demo for a more streamlined experience.           |
| [Zoetrope 5](https://colab.research.google.com/drive/1LpEbICv1mmta7Qqic1IcRTsRsq7UKRHM#scrollTo=iKP0tnHaiTyl)                                 | [@classpectanon](https://twitter.com/classpectanon) | Has advanced parameters for  more controlled AI art generation. I haven't tried this yet, but it may be good to flesh your artwork more.       |
| [VQGAN+CLIP Python coomand-line interface](https://colab.research.google.com/drive/1LpEbICv1mmta7Qqic1IcRTsRsq7UKRHM#scrollTo=iKP0tnHaiTyl)                                 | [@nerdyrodent](https://github.com/nerdyrodent) | Not a Google Colab notebook, but a Github repo that you can fork and run locally. Provides a command-line interface to generate AI-art on the fly.        |
| [VQGAN+CLIP (z+quantize method with augmentations)](https://colab.research.google.com/drive/1_4Jl0a7WIJeqy5LTjPJfZOwMZopG5C-W?usp=sharing)                                 | [@somewheresy](https://twitter.com/somewheresy) | It seems to be the first English-translated notebook of Katherine Crowson ([@RiversHaveWings](https://twitter.com/RiversHaveWings)).        |
| [Nightcafe Studio](https://creator.nightcafe.studio/text-to-image-art)                                 | [NightCafe Studio](https://twitter.com/somewheresy) | Not a Colab notebook, but rather a managed service where you need to setup an account. I can't comment how different the outputs are compared to the Colab notebooks.  |


## CLIP-guided art generators


These aren't necessarily VQGAN implementations, but can produce AI art
nonetheless:


| Name                                                                                                                                          | Author                                              | Description / Features                                                  |
|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------|-------------------------------------------------------------------------|
| [The Big Sleep: BigGAN x CLIP](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing)                    | [@advadnoun](https://twitter.com/advadnoun)             | Uses a CLIP-guided BigGAN generator. I can't comment on the quality of the outputs, but this is exciting to try as well!  |
| [Aleph-Image](https://colab.research.google.com/drive/1Q-TbYvASMPRMXCOQjkxxf72CXYjR_8Vp?usp=sharing)                    | [@advadnoun](https://twitter.com/advadnoun)             | Uses a CLIP-guided DALL-E decoder. Try it out for more interesting results!  |

The common denominator across these works is that they are guided by [OpenAI's
CLIP](https://openai.com/blog/clip/) so that the image matches the text
description. For more CLIP-guided projects, check out this [Reddit post from
February](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/).


## Resources

If you wish to learn more about VQGAN and CLIP, I suggest reading the
following:
- [**Alien Dreams: An Emerging Art Scene by Charlie Snell**](https://ml.berkeley.edu/blog/posts/clip-art/): gives a good overview and history of the recent AI Art scene. Traces its roots from the introduction of CLIP and its pairing of VQGAN today.
- [**The Illustrated VQGAN**](/notebook/2021/08/08/clip-vqgan/): by yours
    truly, here I tried to explain how VQGAN works in a conceptual level. It
    starts with how images are "perceived" then ends with the whole VQGAN
    system.

Of course, nothing beats reading the original papers themselves:

1. <a id="esser2021vqgan">Esser, P., Rombach, R. and Ommer, B.</a>, 2021. Taming transformers for high-resolution image synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition* (pp. 12873-12883).
1. <a id="radford2021clip">Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J. and Krueger, G.</a>, 2021. Learning transferable visual models from natural language supervision. *arXiv preprint arXiv:2103.00020*.

> Did I miss anything? Just comment below!
