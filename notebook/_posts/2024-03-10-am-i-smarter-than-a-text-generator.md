---
layout: post
type: post
title: "Am I smarter than a text generator?"
date: 2024-03-10
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [llm, humaneval, evaluation, annotation, prodigy, nlp, gpt, qa, hellaswag, winogrande]
---

<span class="firstcharacter">N</span>owadays, it's common to see researchers evaluate large language models on "very human" tasks such as reasoning, logic, or math.
Unlike linguistic tasks such as NER and POS tagging, these *human-like* tasks assess skills that we use everyday.
Several benchmark datasets (e.g., HellaSwag, LogiQA, Winogrande, etc.) were even built to examine these skills. 
As a human, I find myself asking: how well will I fare on these benchmarks?

The main purpose of this blog post is to perform a proper human evaluation using myself as a test subject. 
However, we can't just compare two accuracies and call it a day.
So, I plan to uncover three things from this small experiment: 

- First, I want to see how well I'll perform when subjected to the same evaluation (task instruction) most LLMs undergo. 
- Second, I want to assess individual examples and determine if they effectively test the skills for which they were designed to evaluate.
- Finally, I want to identify cases where the model's response disagrees with mine, and verify if this disparity is just an annotation error, a random fluke, or an erroneous attribute of the data sample.

## Annotating multi-choice QA datasets

<!-- experimental setup -->

## Overall results

## Annotation notes for each dataset

### HellaSwag

### PIQA

### Winogrande


### LogiQA

### TruthfulQA

## Final thoughts
