---
layout: post
type: post
title: "Study notes on parameter-efficient finetuning techniques"
date: 2023-06-09
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [langchain, nlp, llm, data annotation, prodigy, natural language processing, chatgpt, gpt-j, gpt-3]
header-img: /assets/png/langchain/header.png
description: |
    Traditional finetuning involves training the parameters of a large language
    model plus a shallower domain-specific network. However, this process is
    inefficient given a limited compute budget. In this blog post, I'll go
    through differrent parameter-efficient finetuning techniques I personally
    like.
excerpt: |
    Traditional finetuning involves training the parameters of a large language
    model plus a shallower domain-specific network. However, this process is
    inefficient given a limited compute budget. In this blog post, I'll go
    through differrent parameter-efficient finetuning techniques I personally
    like.
---