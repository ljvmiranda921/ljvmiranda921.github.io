---
layout: post
type: post
title: "Study notes on parameter-efficient finetuning techniques"
date: 2023-05-15
category: notebook
comments: true
author: "LJ MIRANDA"
published: true
tags: [langchain, nlp, llm, data annotation, prodigy, natural language processing, chatgpt, gpt-j, gpt-3]
header-img: /assets/png/langchain/header.png
description: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
excerpt: |
    Traditional finetuning involves training the parameters of a large language
    model with a shallower domain-specific network. However, this approach
    requires a large compute budget unavailable to most organizations. In this blog
    post, I'll go through differrent parameter-efficient finetuning techniques I
    personally like.
---

<!-- the problem -->



<!-- adapter networks -->


<!-- prefix tuning -->


<!-- mixture of experts -->