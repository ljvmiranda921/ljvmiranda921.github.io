<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A Two-Layer Neural Network from scratch</title>
  <meta name="description" content="In this exercise, a two-layer fully-connected artificial neural network (ANN) was developed in order to perform classification in the CIFAR-10 dataset. The f...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/notebook/2017/02/17/artificial-neural-networks/">
  <link rel="alternate" type="application/rss+xml" title="Lj Miranda" href="/feed.xml">
  <link rel="icon" type="image/png" href="/assets/favicon.ico">
  
  
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
   

  

  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>A Two-Layer Neural Network from scratch | Lj Miranda</title>
<meta property="og:title" content="A Two-Layer Neural Network from scratch" />
<meta name="author" content="LJ MIRANDA" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this exercise, a two-layer fully-connected artificial neural network (ANN) was developed in order to perform classification in the CIFAR-10 dataset. The full-implementation is done through the following steps" />
<meta property="og:description" content="In this exercise, a two-layer fully-connected artificial neural network (ANN) was developed in order to perform classification in the CIFAR-10 dataset. The full-implementation is done through the following steps" />
<link rel="canonical" href="http://localhost:4000/notebook/2017/02/17/artificial-neural-networks/" />
<meta property="og:url" content="http://localhost:4000/notebook/2017/02/17/artificial-neural-networks/" />
<meta property="og:site_name" content="Lj Miranda" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-02-17T00:00:00+09:00" />
<script type="application/ld+json">
{"name":null,"description":"In this exercise, a two-layer fully-connected artificial neural network (ANN) was developed in order to perform classification in the CIFAR-10 dataset. The full-implementation is done through the following steps","url":"http://localhost:4000/notebook/2017/02/17/artificial-neural-networks/","headline":"A Two-Layer Neural Network from scratch","dateModified":"2017-02-17T00:00:00+09:00","datePublished":"2017-02-17T00:00:00+09:00","sameAs":null,"@type":"BlogPosting","author":{"@type":"Person","name":"LJ MIRANDA"},"image":null,"publisher":null,"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/notebook/2017/02/17/artificial-neural-networks/"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    <a class="site-title" href="/">Lj Miranda</a>
  
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
            <a class="page-link" href="/downloads/LMiranda.pdf">CV</a>
            <a class="page-link" href="/research/">Research</a>
            <a class="page-link" href="/projects/">Projects</a>
            <a class="page-link" href="/notebook/">Notebook</a>
        </div>
      </nav>
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">A Two-Layer Neural Network from scratch</h1>
    <p class="post-meta">
      <time datetime="2017-02-17T00:00:00+09:00" itemprop="datePublished">
        
        Feb 17, 2017
      </time>
      
        • <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">LJ MIRANDA</span></span>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In this exercise, a two-layer fully-connected artificial neural network (ANN) was developed in order to perform classification in the CIFAR-10 dataset. The full-implementation is done through the following steps<!--more-->:</p>

<ul>
  <li><a href="#toy">Toy model creation</a></li>
  <li><a href="#implementation">ANN Implementation</a>
    <ul>
      <li><a href="#architecture">Architecture set-up</a></li>
      <li><a href="#loss">Forward Pass: Loss computation</a></li>
      <li><a href="#gradient">Backward Pass: Gradient Computation</a></li>
      <li><a href="#batch">Batch Training</a></li>
      <li><a href="#prediction">Prediction</a></li>
    </ul>
  </li>
  <li><a href="#testing">Toy model testing</a></li>
  <li><a href="#training">Training</a></li>
  <li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="#results">Results</a></li>
</ul>

<h2 id="-toy-model-creation"><a name="toy"></a> Toy Model Creation</h2>
<p>It is first important to build a small neural network in order to test our loss and gradient computations. This can then save us time and effort when debugging our code. Here, we will create a toy model, and a toy dataset in order to check our implementations:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">input_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_inputs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="k">def</span> <span class="nf">init_toy_model</span><span class="p">():</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">init_toy_data</span><span class="p">():</span>
  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">X</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">input_size</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">init_toy_model</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">init_toy_data</span><span class="p">()</span>
</code></pre>
</div>

<p>Remember that <code class="highlighter-rouge">np.random.seed</code> was used so that our experiments are repeatable. This function makes the random numbers predictable. Thus, if we have a seed reset, the same numbers will appear every time. Again, the reason we do this is because we are implementing code that uses random numbers (i.e., <code class="highlighter-rouge">np.random.randn</code>) and if we want to debug this, we need this random number generator to output the same numbers <em>in the meantime</em>. The <a href="http://stackoverflow.com/questions/21494489/what-does-numpy-random-seed0-do">answer</a> of John1024 in StackOverflow is also a good explanation of this function.</p>

<h2 id="-artificial-neural-network-ann-implementation"><a name="implementation"></a> Artificial Neural Network (ANN) Implementation</h2>
<p>In this section, we will implement the forward and backward passes of the ANN, and then write code for batch training and prediction. But first, let us examine the architecture of the neural net.</p>

<h3 id="-architecture-set-up"><a name="architecture"></a> Architecture set-up</h3>
<p>The neural network architecture can be seen below:</p>

<p style="text-align: center;"><img src="/assets/png/cs231n-ann/archi.png" alt="Neural Network Architecture" width="420px" /><br />
<strong>Figure 1:</strong> <em>Articificial Neural Network Architecture</em></p>

<p>There are two layers in our neural network (note that the counting index starts with the first hidden layer up to the output layer). Moreover, the topology between each layer is fully-connected. For the hidden layer, we have ReLU nonlinearity, whereas for the output layer, we have a Softmax loss function.</p>

<p>The “vertical size” of the neural network for the input and output layers is dependent on the CIFAR-10 input and classes respectively, while the hidden layer is arbitrarily set.</p>

<p>Thus, given an input dimension of <code class="highlighter-rouge">D</code>, a hidden layer dimension of <code class="highlighter-rouge">H</code>, and number of classes <code class="highlighter-rouge">C</code>, we have the following weight and bias shapes:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>W1: First layer weights; has shape (D, H)
b1: First layer biases; has shape (H,)
W2: Second layer weights; has shape (H, C)
b2: Second layer biases; has shape (C,)
</code></pre>
</div>

<h3 id="-forward-pass-loss-computation"><a name="loss"></a> Forward Pass: Loss computation</h3>
<p>Before we compute the loss, we need to first perform a forward pass. Because we are implementing a fully-connected layer, the forward pass is simply a straightforward dot matrix operation. We then compute for the pre-activation values <code class="highlighter-rouge">z1</code> and <code class="highlighter-rouge">z2</code> for the hidden and output layers respectively, and the activation for the first layer.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># First layer pre-activation</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>

<span class="c"># First layer activation</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span>

<span class="c"># Second layer pre-activation</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>

<span class="n">scores</span> <span class="o">=</span> <span class="n">z2</span>
</code></pre>
</div>

<p>The <code class="highlighter-rouge">scores</code> variable keeps the pre-activation values for the output layer. We will be using this in a while to find the activation values in the output layer, and consequently the cross-entropy loss.</p>

<p>So for the second-layer activation, we have:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># Second layer activation</span>
<span class="n">exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
<span class="n">a2</span> <span class="o">=</span> <span class="n">exp_scores</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre>
</div>

<p>And to compute for the loss, we perform the following code:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">corect_logprobs</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a2</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span>
<span class="n">data_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">corect_logprobs</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span>
<span class="n">reg_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">*</span> <span class="n">W2</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">data_loss</span> <span class="o">+</span> <span class="n">reg_loss</span>
</code></pre>
</div>

<h3 id="-backward-pass-gradient-computation"><a name="gradient"></a> Backward Pass: Gradient Computation</h3>
<p>We now implement the backward pass, where we compute the derivatives of the weights and biases and propagate them across the network. In this way, the network gets a feel of the contributions of each individual units, and adjusts itself accordingly so that the weights and biases are optimal.</p>

<p>We first compute for the gradients, thus we have:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">dscores</span> <span class="o">=</span> <span class="n">a2</span>
<span class="n">dscores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="n">dscores</span> <span class="o">/=</span> <span class="n">N</span>
</code></pre>
</div>

<p>And then we propagate them back to our network:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># W2 and b2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dscores</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c"># Propagate to hidden layer</span>
<span class="n">dhidden</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="c"># Backprop the ReLU non-linearity</span>
<span class="n">dhidden</span><span class="p">[</span><span class="n">a1</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c"># Finally into W,b</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dhidden</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dhidden</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre>
</div>

<p>We should not also forget to regularize our gradients:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</code></pre>
</div>

<h3 id="-batch-training"><a name="batch"></a> Batch Training</h3>
<p>In our <code class="highlighter-rouge">TwoLayerNet</code> class, we also implement a <code class="highlighter-rouge">train()</code> function that trains the neural network using stochastic gradient descent. First, we create a random minibatch of training data and labels, then we store them in <code class="highlighter-rouge">X_batch</code> and <code class="highlighter-rouge">Y_batch</code> respectively:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">sample_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">X_batch</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>
<span class="n">y_batch</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">sample_indices</span><span class="p">]</span>
</code></pre>
</div>

<p>And then we update our parameters in our network.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span>
<span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
<span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
<span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
</code></pre>
</div>

<h3 id="prediction">Prediction</h3>
<p>Lastly, we implement a <code class="highlighter-rouge">predict()</code> function that classifies our inputs with respect to the scores and activations found after the output layer. We simply make a forward pass for the input, and then get the maximum of the scores that was found.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span>
<span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z1</span><span class="p">)</span> <span class="c"># pass through ReLU activation function</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre>
</div>

<h2 id="-toy-model-testing"><a name="testing"></a> Toy model testing</h2>
<p>Once we’ve implemented our functions, we can then test them to see if they are working properly. The IPython notebook tests our implementation by checking our computed scores with respect to the <code class="highlighter-rouge">correct scores</code> hardcoded in the program.</p>

<p>In the first test, we got a difference of <code class="highlighter-rouge">3.68027206479e-08</code></p>

<div class="highlighter-rouge"><pre class="highlight"><code>Your scores:
[[-0.81233741 -1.27654624 -0.70335995]
 [-0.17129677 -1.18803311 -0.47310444]
 [-0.51590475 -1.01354314 -0.8504215 ]
 [-0.15419291 -0.48629638 -0.52901952]
 [-0.00618733 -0.12435261 -0.15226949]]

correct scores:
[[-0.81233741 -1.27654624 -0.70335995]
 [-0.17129677 -1.18803311 -0.47310444]
 [-0.51590475 -1.01354314 -0.8504215 ]
 [-0.15419291 -0.48629638 -0.52901952]
 [-0.00618733 -0.12435261 -0.15226949]]
</code></pre>
</div>

<p>For the second and third tests, we check the difference between the loss and the correct loss, as well as the relative error between our backward passes:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>Difference between your loss and correct loss:
1.79856129989e-13


b2 max relative error: 4.447635e-11
W1 max relative error: 3.561318e-09
W2 max relative error: 3.440708e-09
b1 max relative error: 2.738421e-09
</code></pre>
</div>

<p>We then test our neural network’s training ability by checking if our loss is decreasing. What we will do is to plot our loss history, and verify that it is actually decreasing.</p>

<p style="text-align: center;"><img src="/assets/png/cs231n-ann/loss_hist.png" alt="Loss history" width="420px" /><br />
<strong>Figure 2:</strong> <em>Plot of loss history on toy dataset</em></p>

<p>It seems like our loss is decreasing and our errors are relatively low. This suggests that our functions are working well and our forward and backward pass implementations
are on point. Because we’re already confident with our code, we can then start training our neural network in the real dataset. I will not show how the data was loaded
and preprocessed because it simply follows a similar structure as with previous exercises. So we will dive straight to training instead.</p>

<h2 id="training">Training</h2>
<p>We first train our neural network with the following parameters (as given in the exercise):</p>

<div class="highlighter-rouge"><pre class="highlight"><code>stats = net.train(X_train, y_train, X_val, y_val,
            num_iters=1000, batch_size=200,
            learning_rate=1e-4, learning_rate_decay=0.95,
            reg=0.5, verbose=True)
</code></pre>
</div>
<p>Given these parameters, we achieve a validation accuracy of <code class="highlighter-rouge">0.287</code>, and it is not particularly good. Remember that our k-Nearest Neighbor classifier can actually perform
relatively the same. In fact, if we plot our loss history and classification accuracy history, we see some troubling signs:</p>

<p style="text-align: center;"><img src="/assets/png/cs231n-ann/wrong.png" alt="Loss history" width="560px" /><br />
<strong>Figure 3:</strong> <em>Plot of loss and classification accuracy on CIFAR-10</em></p>

<p>As of now, we can see that our loss function is decreasing in a non-linear fashion. It looks weird and it may suggest a low learning rate. Moreover, we can also see that there is no gap between the training and validation model, and this may suggest that our model has very low capacity.</p>

<p>We can also visualize the weights of our network, in this case, we arrive at the following:</p>

<p style="text-align: center;"><img src="/assets/png/cs231n-ann/bad_weights.png" alt="NN Weights" width="560px" /><br />
<strong>Figure 4:</strong> <em>Visualization of ANN Weights using default parameters</em></p>

<p>We can see from here that the network is learning a set of weights that are very similar to one another. One can see a lot of car classes in this network. Because of this “homogeneity,” any input is often misclassified by the network.</p>

<p>What we will be doing next is to tune our hyperparameters, and try to achieve a good score.</p>

<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>
<p>We then tune our parameters by sweeping through different values of learning rates and regularization strengths. For this implementation, I would like to attribute <a href="https://github.com/MyHumbleSelf/cs231n/tree/master/assignment1">MyHumbleSelf</a>’s solution as my basis for finding a good combination of parameters.</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="n">best_val</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">best_stats</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">]</span>
<span class="n">regularization_strengths</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">rs</span> <span class="ow">in</span> <span class="n">regularization_strengths</span><span class="p">:</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

        <span class="c"># Train the network</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span>
                    <span class="n">num_iters</span><span class="o">=</span><span class="n">iters</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">learning_rate_decay</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
                    <span class="n">reg</span><span class="o">=</span><span class="n">rs</span><span class="p">)</span>

        <span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">acc_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">y_train_pred</span><span class="p">)</span>
        <span class="n">y_val_pred</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
        <span class="n">acc_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_val</span> <span class="o">==</span> <span class="n">y_val_pred</span><span class="p">)</span>

        <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">rs</span><span class="p">)]</span> <span class="o">=</span> <span class="p">(</span><span class="n">acc_train</span><span class="p">,</span> <span class="n">acc_val</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">best_val</span> <span class="o">&lt;</span> <span class="n">acc_val</span><span class="p">:</span>
            <span class="n">best_stats</span> <span class="o">=</span> <span class="n">stats</span>
            <span class="n">best_val</span> <span class="o">=</span> <span class="n">acc_val</span>
            <span class="n">best_net</span> <span class="o">=</span> <span class="n">net</span>

<span class="c"># Print out results.</span>
<span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
    <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">results</span><span class="p">[(</span><span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">)]</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'lr </span><span class="si">%</span><span class="s">e reg </span><span class="si">%</span><span class="s">e train accuracy: </span><span class="si">%</span><span class="s">f val accuracy: </span><span class="si">%</span><span class="s">f'</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">lr</span><span class="p">,</span> <span class="n">reg</span><span class="p">,</span> <span class="n">train_accuracy</span><span class="p">,</span> <span class="n">val_accuracy</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="s">'best validation accuracy achieved during cross-validation: </span><span class="si">%</span><span class="s">f'</span> <span class="o">%</span> <span class="n">best_val</span><span class="p">)</span>
</code></pre>
</div>

<p>From here, we were able to ramp up our validation accuracy up to <code class="highlighter-rouge">0.497000</code>. This is particularly good compared to our SVM and Softmax Implementation. If we visualize the learned weights, we can obtain the following figure:</p>

<p style="text-align: center;"><img src="/assets/png/cs231n-ann/good_weights.png" alt="NN Weights" width="560px" /><br />
<strong>Figure 5:</strong> <em>Visualization of ANN Weights after tuning</em></p>

<p>We can then see that the weights are more heterogeneous. This can then accommodate more classes for a given set of inputs. Personally, what I like about this is that it looks like an acid trip.</p>

<h1 id="results">Results</h1>
<p>Lastly, we implement our learned weights into our network, and check the test accuracy. For my implementation, I got <code class="highlighter-rouge">0.496</code></p>

  </div>

  
    

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Lj Miranda</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              Lj Miranda
            
            </li>
            
            <li><a href="mailto:ljvmiranda@gmail.com">ljvmiranda@gmail.com</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
          <ul class="social-media-list">
    
    
    
    <li><a href="https://github.com/ljvmiranda921"><svg class="svg-icon"><use xlink:href="/assets/icons.svg#github"></use></svg> <span class="username">Github</span></a></li>
    
    <li><a href="https://www.linkedin.com/in/lesterjamesmiranda"><svg class="svg-icon"><use xlink:href="/assets/icons.svg#linkedin"></use></svg> <span class="username">Linkedin</span></a></li>
    
    
    
    
    <li><a href="/feed.xml"><svg class="svg-icon"><use xlink:href="/assets/icons.svg#rss"></use></svg> <span>RSS</span></a></li>
</ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Some notes on software development, data science, machine learning, and research.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
